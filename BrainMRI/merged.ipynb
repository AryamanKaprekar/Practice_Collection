{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhwbRG9SOEfQ"
      },
      "source": [
        "# **Combining Pretrained Model (Resnet18) and Custom Model (TinyVGG) for better results, using Brain MRI dataset from Kaggle**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQebOQ_ok7Ag",
        "outputId": "0f175236-1f8b-4faf-824e-749f2b0c00fb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q52uiWcVlLlT",
        "outputId": "558f296a-0264-4d1a-c7f9-7cfe89a9f020"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import kagglehub\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Download dataset using kagglehub\n",
        "path = kagglehub.dataset_download(\"rm1000/brain-tumor-mri-scans\")\n",
        "print(f\"Path to dataset: {path}\")\n",
        "\n",
        "# Define paths for training and test sets\n",
        "train_path = \"/content/train\"\n",
        "test_path = \"/content/test\"\n",
        "\n",
        "# Check if train and test sets already exist\n",
        "if os.path.exists(train_path) and os.path.exists(test_path):\n",
        "    print(\"Train and test sets already exist.\")\n",
        "else:\n",
        "    # Create directories for training and testing sets\n",
        "    os.makedirs(train_path, exist_ok=True)\n",
        "    os.makedirs(test_path, exist_ok=True)\n",
        "\n",
        "    # Split each classification into training and test sets\n",
        "    for class_name in os.listdir(path):\n",
        "        class_dir = os.path.join(path, class_name)\n",
        "\n",
        "        # Skip if not a directory\n",
        "        if not os.path.isdir(class_dir):\n",
        "            continue\n",
        "\n",
        "        # List all images in the class directory\n",
        "        images = os.listdir(class_dir)\n",
        "\n",
        "        # Split into training and testing sets\n",
        "        train_images, test_images = train_test_split(images, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Create subdirectories for the class in train and test folders\n",
        "        os.makedirs(os.path.join(train_path, class_name), exist_ok=True)\n",
        "        os.makedirs(os.path.join(test_path, class_name), exist_ok=True)\n",
        "\n",
        "        # Move training images\n",
        "        for image in train_images:\n",
        "            shutil.move(os.path.join(class_dir, image), os.path.join(train_path, class_name, image))\n",
        "\n",
        "        # Move testing images\n",
        "        for image in test_images:\n",
        "            shutil.move(os.path.join(class_dir, image), os.path.join(test_path, class_name, image))\n",
        "\n",
        "    print(\"Dataset successfully divided into training and test sets!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW8mO2BUlQt-",
        "outputId": "5dca21af-3d43-40d2-ed9f-a27238c41d83"
      },
      "outputs": [],
      "source": [
        "transform= transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "train_data=datasets.ImageFolder(root=train_path, transform=transform)\n",
        "test_data=datasets.ImageFolder(root=test_path, transform=transform)\n",
        "print(len(train_data),len(test_data))\n",
        "print(train_data.class_to_idx)\n",
        "print(test_data.class_to_idx)\n",
        "train_dataloader=torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_dataloader=torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNIofXjDK7LC",
        "outputId": "43448770-f75d-4c18-f6e6-08a40a4bdc56"
      },
      "outputs": [],
      "source": [
        "# Define cnn model as per your structure\n",
        "class cnnmodel(nn.Module):\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
        "        super().__init__()\n",
        "        self.conv_block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape, out_channels=hidden_units, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.conv_block_2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=hidden_units * 16 * 16, out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block_1(x)\n",
        "        x = self.conv_block_2(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Define the hybrid model combining ResNet-18 and TinyVGG\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, output_shape: int):\n",
        "        super(HybridModel, self).__init__()\n",
        "\n",
        "        # Load pre-trained ResNet-18 and remove the final layers\n",
        "        self.resnet = models.resnet18(pretrained=True)\n",
        "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-2])  # Remove FC and avgpool layers\n",
        "\n",
        "        # Initialize cnn model with matching input channels and output shape\n",
        "        self.cnnmodel = cnnmodel(input_shape=512, hidden_units=128, output_shape=output_shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features with ResNet-18\n",
        "        x = self.resnet(x)\n",
        "\n",
        "        # Interpolate to 64x64 to fit TinyVGG’s input requirements\n",
        "        x = F.interpolate(x, size=(64, 64), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Pass through cnn model for the final classification\n",
        "        x = self.cnnmodel(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Instantiate the hybrid model with desired output shape\n",
        "output_shape = 4 # for example, 10 classes\n",
        "model = HybridModel(output_shape=output_shape).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SdzmMnYl5Qr"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          class_names: list[str],\n",
        "          epochs: int):\n",
        "    results = {\"train_loss\": [], \"train_acc\": []}\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        model.train()\n",
        "        train_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for batch, (X, y) in enumerate(train_dataloader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            y_pred = model(X)\n",
        "            loss = loss_fn(y_pred, y)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Predictions\n",
        "            y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "            correct += (y_pred_class == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "            # Collect predictions and labels for metrics at the end\n",
        "            all_preds.extend(y_pred_class.cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "        # Calculate loss and accuracy for the epoch\n",
        "        train_acc = correct / total\n",
        "        train_loss /= len(train_dataloader)\n",
        "\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "\n",
        "    # Calculate and print average metrics after training\n",
        "    print(\"\\nEvaluating on training data after last epoch...\")\n",
        "\n",
        "    # Calculate precision, recall, and F1 score\n",
        "    metrics = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n",
        "    avg_precision = metrics[\"weighted avg\"][\"precision\"]\n",
        "    avg_recall = metrics[\"weighted avg\"][\"recall\"]\n",
        "    avg_f1_score = metrics[\"weighted avg\"][\"f1-score\"]\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "    disp.plot(cmap=\"Blues\", xticks_rotation=\"vertical\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    # Print final metrics\n",
        "    print(f\"Avg Precision: {avg_precision:.4f} | Avg Recall: {avg_recall:.4f} | Avg F1-Score: {avg_f1_score:.4f}\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "722a2f7a56dd426d80e1cac5bb8a0770",
            "43e9fa6c114241b1830f58acbcacb4eb",
            "4bc3d9f2c77945429dd7c584e0f6bcd3",
            "658fedd06f1e4a61904063ec067187e8",
            "d7447ae590364dd98ff51de58ffd2df7",
            "eb9010b74233496cbeb3d156cce8a4d8",
            "d514761644b54b1d89ff8298a1aea614",
            "2fe77fe64a794a4db9511449a4d19f5c",
            "5de7bb25d5ea449fba5fa9503bc9c5cc",
            "8279af4a2d0a43aab6cc5a6d1ff254db",
            "6f29e0a03b044f688da73ce1a606d37f"
          ]
        },
        "id": "vRtB7nosKywq",
        "outputId": "81fc14de-25c5-47f3-f51c-fb9c19363430"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-4, weight_decay=0.0001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "class_names = train_data.classes\n",
        "from timeit import default_timer as timer\n",
        "start_time=timer()\n",
        "train_results = train(model=model,\n",
        "                      train_dataloader=train_dataloader,\n",
        "                      optimizer=optimizer,\n",
        "                      loss_fn=loss_fn,\n",
        "                      class_names=class_names,\n",
        "                      epochs=50)\n",
        "end_time=timer()\n",
        "print(f\"Total training time {end_time - start_time:.3f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXeRKNbS9QC_",
        "outputId": "dc9f8f4f-c798-4585-df22-666c0e0ab613"
      },
      "outputs": [],
      "source": [
        "def save_model(model: torch.nn.Module, path: str):\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Model saved to {path}\")\n",
        "\n",
        "# Example usage\n",
        "model_save_path = \"/content/drive/MyDrive/Dataset/model.pth\"\n",
        "save_model(model, model_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZQ4G_s_EQck",
        "outputId": "67425e75-6f76-42d6-e974-85dae1360cef"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "def load_model(model: torch.nn.Module, path: str, device=\"cpu\"):\n",
        "    model.load_state_dict(torch.load(path, map_location=device))\n",
        "    model.to(device)\n",
        "    print(f\"Model loaded from {path}\")\n",
        "    return model\n",
        "model_load_path = \"/content/drive/MyDrive/Dataset/model.pth\"\n",
        "# Example usage\n",
        "loaded_model = load_model(model, model_load_path, device=device)\n",
        "def test_and_save_images(model: torch.nn.Module,\n",
        "                         dataloader: torch.utils.data.DataLoader,\n",
        "                         output_folder: str,\n",
        "                         class_names: list[str],\n",
        "                         transform=None,\n",
        "                         device=device):\n",
        "\n",
        "    model.eval()\n",
        "    os.makedirs(output_folder, exist_ok=True)  # Create the output folder if it doesn't exist\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # Get predictions\n",
        "            y_pred = model(X)\n",
        "            y_pred_prob = torch.softmax(y_pred, dim=1)\n",
        "            y_pred_class = torch.argmax(y_pred_prob, dim=1)\n",
        "\n",
        "            # Loop through each image in the batch\n",
        "            for i in range(len(X)):\n",
        "                # Convert the image tensor to a NumPy array\n",
        "                img = X[i].cpu().detach()\n",
        "\n",
        "                if transform:\n",
        "                    img = transform(img)\n",
        "\n",
        "                img = img.permute(1, 2, 0).numpy()  # Convert to HxWxC format for displaying\n",
        "                img = np.clip((img * 255), 0, 255).astype(np.uint8)  # Denormalize and clip the values to [0, 255]\n",
        "\n",
        "                # Prepare the prediction title\n",
        "                pred_label = class_names[y_pred_class[i].item()]\n",
        "                pred_prob = y_pred_prob[i, y_pred_class[i].item()].item()\n",
        "                title = f\"Pred: {pred_label} | Prob: {pred_prob:.3f}\"\n",
        "\n",
        "                # Save image with title as the name\n",
        "                image_filename = f\"{output_folder}/image_{batch * len(X) + i}_pred_{pred_label}_prob_{pred_prob:.3f}.png\"\n",
        "\n",
        "                # Convert NumPy array to PIL image using Image.fromarray() after ensuring uint8 type\n",
        "                pil_image = Image.fromarray(img)\n",
        "                pil_image.save(image_filename)\n",
        "\n",
        "    print(f\"Images with predictions and probabilities saved to {output_folder}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBz1mcmZDgzN",
        "outputId": "f61ea89c-9ac1-494e-c932-8f04c3fe7f4d"
      },
      "outputs": [],
      "source": [
        "output_folder = \"/content/test_results\"  # Folder where images will be saved\n",
        "class_names = train_data.classes  # Assuming this is your class names\n",
        "transform = transforms.Compose([transforms.Resize((224, 224))])  # Optional: Resize image if needed\n",
        "\n",
        "test_and_save_images(model=loaded_model,\n",
        "                     dataloader=test_dataloader,\n",
        "                     output_folder=output_folder,\n",
        "                     class_names=class_names,\n",
        "                     transform=transform,\n",
        "                     device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "VIWSG7MIIYa9",
        "outputId": "86749ebe-f57c-4c2f-f07b-b309e5136d99"
      },
      "outputs": [],
      "source": [
        "\"\"\"import shutil\n",
        "\n",
        "# Path to the folder you want to delete\n",
        "folder_path = \"/content/test_results\"\n",
        "\n",
        "# Delete the folder and all its contents\n",
        "shutil.rmtree(folder_path)\n",
        "\n",
        "print(f\"Folder '{folder_path}' and all its contents have been deleted.\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "PJAc63enLnNq",
        "outputId": "231ee0db-afab-4c47-87d5-55fa5cac61a9"
      },
      "outputs": [],
      "source": [
        "custom_image_path=\"/content/testing.png\"\n",
        "custom_image_transform=transforms.Compose([\n",
        "    transforms.Resize(size=(224,224))\n",
        "])\n",
        "def pred_and_plot_image(model:torch.nn.Module,\n",
        "                         image_path:str,\n",
        "                         class_names:list[str]=None,\n",
        "                         transform=None,\n",
        "                         device=device):\n",
        "  target_image=torchvision.io.read_image(str(image_path)).type(torch.float32)/255.0\n",
        "  if transform:\n",
        "    target_image=transform(target_image)\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    target_image_pred=model(target_image.unsqueeze(0).to(device))\n",
        "  target_image_pred_prob=torch.softmax(target_image_pred, dim=1)\n",
        "  target_image_pred_label=torch.argmax(target_image_pred_prob, dim=1)\n",
        "  plt.imshow(target_image.squeeze().permute(1,2,0))\n",
        "  if class_names:\n",
        "    title=f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_prob.max():.3f}\"\n",
        "  else:\n",
        "    title=f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_prob.max():.3f}\"\n",
        "  plt.title(title)\n",
        "  plt.axis(False)\n",
        "pred_and_plot_image(model=loaded_model,\n",
        "                  image_path=custom_image_path,\n",
        "                  class_names=train_data.classes,\n",
        "                  transform=custom_image_transform,\n",
        "                  device=device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMMUkDuX4T8ftqyl3aUwh0o",
      "gpuType": "T4",
      "include_colab_link": true,
      "mount_file_id": "https://github.com/AryamanKaprekar/Practice_Collection/blob/main/merged.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2fe77fe64a794a4db9511449a4d19f5c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43e9fa6c114241b1830f58acbcacb4eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb9010b74233496cbeb3d156cce8a4d8",
            "placeholder": "​",
            "style": "IPY_MODEL_d514761644b54b1d89ff8298a1aea614",
            "value": "100%"
          }
        },
        "4bc3d9f2c77945429dd7c584e0f6bcd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fe77fe64a794a4db9511449a4d19f5c",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5de7bb25d5ea449fba5fa9503bc9c5cc",
            "value": 50
          }
        },
        "5de7bb25d5ea449fba5fa9503bc9c5cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "658fedd06f1e4a61904063ec067187e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8279af4a2d0a43aab6cc5a6d1ff254db",
            "placeholder": "​",
            "style": "IPY_MODEL_6f29e0a03b044f688da73ce1a606d37f",
            "value": " 50/50 [51:31&lt;00:00, 61.77s/it]"
          }
        },
        "6f29e0a03b044f688da73ce1a606d37f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "722a2f7a56dd426d80e1cac5bb8a0770": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43e9fa6c114241b1830f58acbcacb4eb",
              "IPY_MODEL_4bc3d9f2c77945429dd7c584e0f6bcd3",
              "IPY_MODEL_658fedd06f1e4a61904063ec067187e8"
            ],
            "layout": "IPY_MODEL_d7447ae590364dd98ff51de58ffd2df7"
          }
        },
        "8279af4a2d0a43aab6cc5a6d1ff254db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d514761644b54b1d89ff8298a1aea614": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7447ae590364dd98ff51de58ffd2df7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb9010b74233496cbeb3d156cce8a4d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
