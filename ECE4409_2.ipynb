{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b19c30-dbee-4e1a-844e-611e55a9e0a6",
   "metadata": {},
   "source": [
    "# <span style='color:Red'>Supervised Learning</span>\n",
    "***\n",
    "* ### Remember that supervised learning is used whenever we want to predict a certain outcome from a given input, and we have examples of input/output pairs.\n",
    "* ### We build a machine learning model from these input/output pairs, which comprise our training set.\n",
    "* ### Our goal is to make accurate predictions for new, never-before-seen data.\n",
    "* ### There are two major types of supervised machine learning problems, called classification and regression.\n",
    "* ### <span style='color:Blue'>Classification:\n",
    "    * #### In classification, the goal is to predict a class label, which is a choice from a predefined list of possibilities.\n",
    "    * #### Classification can be a binary classification, which distinguishes between exactly two classes, and multiclass classification, which is a classification between more than two classes.\n",
    "    * #### You can think of binary classification as trying to answer a yes/no question. Classifying emails as either spam or not spam is an example of a binary classification problem. In this binary classification task, the yes/no question being asked would be “Is this email spam?”\n",
    "    * #### The iris example, discussed earlier, is an example of a multiclass classification problem.\n",
    "    * #### Another multiclass classification example is predicting what language a website is in from the text on the website. The classes here would be a pre-defined list of possible languages. \n",
    "* ### <span style='color:Blue'>Regression:\n",
    "    * #### For regression tasks, the goal is to predict a continuous number, or a floating-point number in programming terms (or real number in mathematical terms).\n",
    "    * #### Predicting a person’s annual income from their education, their age, and where they live is an example of a regression task. When predicting income, the predicted value is an amount, and can be any number in a given range.\n",
    "    * #### Another example of a regression task is predicting the yield of a corn farm given attributes such as previous yields, weather, and number of employees working on the farm. The yield again can be an arbitrary number.\n",
    "    * #### An easy way to distinguish between classification and regression tasks is to ask whether there is some kind of continuity in the output. If there is continuity between possible outcomes, then the problem is a regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020ef8db-af42-449a-bb99-338d6a94d253",
   "metadata": {},
   "source": [
    "# <span style='color:Red'>Classification: </span>\n",
    "***\n",
    "## <span style='color:Blue'>Learning a class from examples</span>      (Ref: section 2.1 from book [[1]](https://erp.metbhujbalknowledgecity.ac.in/StudyMaterial/01VM092015008350131.pdf))\n",
    "* ### Goal: To learn the class, $\\mathbf{'C'}$, of a family car\n",
    "    * #### Learning class $\\mathbf{C}$ means that identifying a decision rule that separates class-$\\mathbf{C}$ objects fron non class-$\\mathbf{C}$ objects. \n",
    "    * #### All family cars are considered as *positive examples*, and the other cars are *negative examples*.\n",
    "    * #### Class learning is finding a description(model) that is shared by all positive examples and none of the negative examples.\n",
    "    * #### Doing this, we can make a prediction: Given a car that we have not seen before, by checking with the description(model) learned, we will be able to say whether it is a family car or not.\n",
    "  <br>\n",
    "* ### Assume that the two features that can separate a family car from other cars are the price and engine power.\n",
    "  <br>\n",
    "* ### Let $\\mathbf{x}$ be the datapoint that holds the features $x_1$(price) and $x_2$(engine power):\n",
    "  $$\n",
    "  \\Large\n",
    "  \\mathbf{x}=\n",
    "  \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix} \\tag 1\n",
    "$$\n",
    "\n",
    "* ### Let $y$ be the label:\n",
    "    * #### $y=1$ if $x$ is a positive example, and $y=0$ if $x$ is a negative example.\n",
    "  <br>\n",
    "* ### Each instance (car) is represented by such an ordered pair $(\\mathbf{x},y)$ and the training set contains $N$ such examples\n",
    "$$ \n",
    " \\Large \\mathbf{X}= \\{\\mathbf{x}^t,y^t\\}_{t=1}^N \\tag 2\n",
    "$$ \n",
    "&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; where t indexes different examples in the set.\n",
    "\n",
    "* ### Our training data can now be plotted in the two-dimensional $(x_1,x_2)$ space where each instance $t$ is a data point at coordinates $(x_1^{t},x_2^{t})$ and its type, namely, positive versus negative, is given by $y^t$ (see figure 2.1)\n",
    "  &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;  <img src=\"images/Class_Learning1.png\" width=\"600\" height=\"300\">\n",
    "<br>\n",
    "<br>\n",
    "* ### After further discussions with the expert and the analysis of the data, we may have reason to believe that for a car to be a family car, its price and engine power should be in a certain range\n",
    "$$ \n",
    " (p_{1} \\le \\ price\\ \\le p_{2})\\ \\&\\&\\ (e_1 \\le engine\\ power \\le e_2) \\tag 3\n",
    "$$ \n",
    "&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for suitable values of $p_1$, $p_2$, $e_1$, and $e_1$\n",
    "    \n",
    "    \n",
    "#### &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; Equation (3) thus assumes class $\\mathbf{C}$ to be a rectangle in the price-engine power space (see figure 2.2)\n",
    "  \n",
    " &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;  <img src=\"images/Class_Learning2.png\" width=\"600\" height=\"300\">\n",
    "  <br>\n",
    "  <br>\n",
    "* ### let $\\mathbf{H}=\\{h_1,h_2,h_3,.....\\}$ be the hypothesis class, where $h_1,h_2,h_3,.....$ are the set of all possible hypotheses(rectangles).\n",
    "* ### Each quadrupole ( $p_1^h,p_2^h,e_1^h,e_2^h$ ) defines one hypothesis, $h$, from $\\mathbf{H}$, and the learning algorithm must find the optimum hypothesis $h\\in \\mathbf{H}$, that approximates $\\mathbf{C}$ as closely as possible, or in otherwords, it needs to find the values of these four parameters given the training set, to include all the positive examples and none of the negative examples\n",
    "    <br>\n",
    "* ### Assume that any hypothesis $h$ that belongs to the set $\\mathbf{H}=\\{h_1,h_2,h_3,.....\\}$ is a function of input $\\mathbf{x}$, and it outputs 1 if the input is identified as a positive example, or 0 if the input is identified as a negative example:\n",
    "$$ \n",
    " h(\\mathbf{x})=\n",
    " \\begin{cases}\n",
    "   1;\\ if\\ h\\ classifies\\ \\mathbf{x}\\ as\\ a\\ positive\\ example\\\\    \n",
    "                                                            \\\\\n",
    "   0;\\ if\\ h\\ classifies\\ \\mathbf{x}\\ as\\ a\\ negative\\ example    \n",
    "\\end{cases}\n",
    "$$ \n",
    "* ### The empirical error $E$ of the hypothesis $h$ learned on the training set $\\large \\mathbf{X}$ is\n",
    "$$  \n",
    "        E(h|\\mathbf{X})\\ =\\ \\mathbf{1}(h(\\mathbf{x}^1)\\ne y^1) + \\mathbf{1}(h(\\mathbf{x}^2)\\ne y^2) + ..... + \\mathbf{1}(h(\\mathbf{x}^N)\\ne y^N)  \n",
    "$$ \n",
    "\n",
    "$$  \n",
    "        E(h|\\mathbf{X}) =  \\sum_{t=1}^{N} \\mathbf{1}(h(\\mathbf{x}^t)\\ne y^t) \n",
    "$$ \n",
    "&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \n",
    " $$ \n",
    " where \\ \n",
    " \\mathbf{1}(a\\ne b)=\n",
    " \\begin{cases}\n",
    "   1;\\ if\\ a\\ne b\\\\    \n",
    "                                                            \\\\\n",
    "   0;\\ if\\ a=b\\\\    \n",
    "\\end{cases}\n",
    "$$\n",
    "&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;   <img src=\"images/Class_Learning3.png\" width=\"500\" height=\"250\">\n",
    "<br>\n",
    "* ### On the training set, many hypotheses can satisfy the requirement of (including all the positive and none of the negative examples) giving the empirical error $E$ zero. But given a future example somewhere close to the boundary between positive and negative examples, different candidate hypotheses may make different predictions.\n",
    "* ### This is the problem of generalization - that is, how well our hypothesis will correctly classify future examples that are not part of the training set.\n",
    "    <br>\n",
    "    &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;   <img src=\"images/Class_Learning4.png\" width=\"500\" height=\"250\">\n",
    "* ### One possibility to improve generalization is:\n",
    "    * #### Find the most specific hypothesis, $\\mathbf{S}$: The tightest rectangle that includes all the positive examples and none of the negative examples (see figure 2.4)\n",
    "    * #### The actual class $\\mathbf{C}$ may be larger than $\\mathbf{S}$ but is never smaller\n",
    "    * ####  Find the most general hypothesis, $\\mathbf{G}$: The largest rectangle we can draw that includes all the positive examples and none of the negative examples (see figure 2.4)\n",
    "    * #### Any $h \\in \\mathbf{H}$ between $\\mathbf{S}$ and $\\mathbf{G}$ is a valid hypothesis with no error, said to be consistent with the training set, and such $h$ make up the version space.\n",
    "    * #### Given another training set $\\mathbf{X}$, the $\\mathbf{S}$, $\\mathbf{G}$, version space, and thus the learned hypothesis, $h$, can be different.\n",
    "    * #### This recommends the training set $\\mathbf{X}$ to be large enough that there is a unique $\\mathbf{S}$ and $\\mathbf{G}$.\n",
    "    * #### It is intuitive to choose $h$ halfway between $\\mathbf{S}$ and $\\mathbf{G}$; this is to increase the margin, which is the distance between the boundary and the instances closest to it (see figure 2.5).\n",
    "&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;   <img src=\"images/Class_Learning5.png\" width=\"500\" height=\"250\">\n",
    "* #### For our error function to have a minimum at $h$ with the maximum margin, we should use an error (loss) function which not only checks whether an instance is on the correct side of the boundary but also how far away it is.\n",
    "* #### That is, instead of $h(\\mathbf{x})$ that returns $0/1$, we need to have a hypothesis that returns a value which carries a measure of the distance to the boundary and we need to have a loss function which uses it, different from 1(.) that checks for the eqality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e986c249-c308-4acc-b421-408b90340685",
   "metadata": {},
   "source": [
    "## <span style='color:Blue'>Noise</span>      (Ref: section 2.4 from book [[1]](https://erp.metbhujbalknowledgecity.ac.in/StudyMaterial/01VM092015008350131.pdf))\n",
    "* ### Noise is any unwanted anomaly in the data and due to noise, the class may be more difficult to learn and zero error may be infeasible with a simple hypothesis class (see figure 2.8).\n",
    "\n",
    "&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;   <img src=\"images/Class_Learning6.png\" width=\"500\" height=\"250\">\n",
    "\n",
    "* ### There are several interpretations of noise:\n",
    "    * #### There may be imprecision in recording the input attributes, which may shift the data points in the input space.\n",
    "    * #### There may be errors in labeling the data points, which may relabel positive instances as negative and vice versa. This is sometimes called teacher noise.\n",
    "    * #### There may be additional attributes, which we have not taken into account, that affect the label of an instance. Such attributes may be hidden or latent in that they may be unobservable. The effect of these neglected attributes is thus modeled as a random component and is included in “noise.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37322816-acf2-4b87-8df5-c541dd79090c",
   "metadata": {},
   "source": [
    "## <span style='color:Blue'>Learning multiple class</span>      (Ref: section 2.5 from book [[1]](https://erp.metbhujbalknowledgecity.ac.in/StudyMaterial/01VM092015008350131.pdf))\n",
    "* ### In our example of learning a family car, we have positive examples belonging to the class family car and negative examples belonging to all other cars. This is a two-class problem.\n",
    "* ### In the general case, we have $K$ classes denoted as $C_{i}, i=1,.....K$, and an input instance belongs to one and exactly one of them.\n",
    "* ### The training set is now of the form:\n",
    "  $$ \n",
    " \\Large \\mathbf{X}= \\{\\mathbf{x}^t,\\mathbf{y}^t\\}_{t=1}^N \n",
    "$$\n",
    "* ### Here the label $\\mathbf{y}$ is of $K$-dimensional vector of the form:\n",
    "$$\n",
    "  \\Large\n",
    "  \\mathbf{y}^t=\n",
    "  \\begin{bmatrix}\n",
    "y^t_1 \\\\\n",
    "y^t_2 \\\\\n",
    "..\\\\\n",
    "..\\\\\n",
    "y^t_K\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "<br>\n",
    "$$ \n",
    " where \\ \n",
    " y^t_i=\n",
    " \\begin{cases}\n",
    "   1;\\ if\\ \\mathbf{x}^t\\in \\mathbf{C}_i\\\\    \n",
    "                                                            \\\\\n",
    "   0;\\ if\\ \\mathbf{x}^t\\in \\mathbf{C}_j,\\ j\\ne\\ i\\\\    \n",
    "\\end{cases}\n",
    "$$\n",
    "<br>\n",
    "* ### An example is given in figure 2.9 with instances from three classes:family car, sports car, and luxury sedan\n",
    "&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;   <img src=\"images/Class_Learning7.png\" width=\"500\" height=\"250\">\n",
    "* ### In machine learning for classification, we would like to learn the boundary separating the instances of one class from the instances of all other classes. Thus we view a K-class classification problem as K two-class problems.\n",
    "* ### Let $h_i$ be the hypothesis that separates class-$C_i$ instances from non class-$C_i$ instances. The training examples belonging to $C_i$ are the positive instances of hypothesis $h_i$ and the examples of all other classes are the negative instances of $h_i$.\n",
    "* ### Thus in a $K$-class problem, we have $K$ hypotheses to learn such that\n",
    " $$ \n",
    "  h_i(\\mathbf{x}^t)=\n",
    " \\begin{cases}\n",
    "   1;\\ if\\ \\mathbf{x}^t\\in \\mathbf{C}_i\\\\    \n",
    "                                                            \\\\\n",
    "   0;\\ if\\ \\mathbf{x}^t\\in \\mathbf{C}_j,\\ j\\ne\\ i\\\\    \n",
    "\\end{cases}\n",
    "$$\n",
    "* ### The total empirical error takes a sum over the predictions of all classes over all instances:\n",
    "$$  \n",
    "        E(\\{h\\}_{i=1}^K|\\mathbf{X})\\ =\\ [\\mathbf{1}(h_1(\\mathbf{x}^1)\\ne y_1^1) + \\mathbf{1}(h_2(\\mathbf{x}^1)\\ne y_2^1) + ..... + \\mathbf{1}(h_K(\\mathbf{x}^1)\\ne y_K^1)]  + [\\mathbf{1}(h_1(\\mathbf{x}^2)\\ne y_1^2) + \\mathbf{1}(h_2(\\mathbf{x}^2)\\ne y_2^2) + ..... + \\mathbf{1}(h_K(\\mathbf{x}^2)\\ne y_K^2)]\n",
    "        + ...... + [\\mathbf{1}(h_1(\\mathbf{x}^N)\\ne y_1^N) + \\mathbf{1}(h_2(\\mathbf{x}^N)\\ne y_2^N) + ..... + \\mathbf{1}(h_K(\\mathbf{x}^N)\\ne y_K^N)]  \n",
    "$$ \n",
    "\n",
    "$$  \n",
    "        E(\\{h_i\\}_{i=1}^K|\\mathbf{X}) =  \\sum_{t=1}^{N} \\sum_{i=1}^{K} \\mathbf{1}(h_i(\\mathbf{x}^t)\\ne y^t) \n",
    "$$ \n",
    "<br>\n",
    "* ### For a given $\\mathbf{x}$, ideally only one of $h_i(\\mathbf{x})$, $i = 1,...,K$ is 1 and we can choose a class. But when no, or two or more, $h_i(x)$ is 1, we cannot choose a class, and this is the case of doubt and the classifier rejects such cases.\n",
    "* ### In our example of learning a family car, we used only one hypothesis and only modeled the positive examples. Any negative example outside is not a family car.\n",
    "* ### Alternatively, sometimes we may prefer to build two hypotheses, one for the positive and the other for the negative instances. This assumes a structure also for the negative instances that can be covered by another hypothesis.\n",
    "* ### Separating family cars from sports cars is such a problem; each class has a structure of its own. The advantage is that if the input is a luxury sedan, we can have both hypotheses decide negative and reject the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7974525b-505c-4508-b122-d0fb2d2cedf2",
   "metadata": {},
   "source": [
    "# <span style='color:Red'>Regression: </span> (Ref: section 2.6 from book [[1]](https://erp.metbhujbalknowledgecity.ac.in/StudyMaterial/01VM092015008350131.pdf))\n",
    "***\n",
    "* ### In classification, given an input, the output that is generated is Boolean; it is a yes/no answer.\n",
    "* ### When the output is a numeric value, what we would like to learn is not a class, $C(x) \\in {0, 1}$, but is a numeric function.\n",
    "* ### In machine learning, the function is not known but we have a training set of examples drawn from it\n",
    "$$ \n",
    " \\Large \\mathbf{X}= \\{\\mathbf{x}^t,y^t\\}_{t=1}^N \n",
    "$$\n",
    "&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; where $y^t\\ \\in \\ \\mathbb{R}$\n",
    "* ### If there is no noise, the task is interpolation. We would like to find the function $f(\\mathbf{x})$ that passes through these points such that we have\n",
    "$$ \n",
    " \\Large y^t= f(\\mathbf{x}^t) \n",
    "$$\n",
    "* ### The empirical error on the training set $\\mathbf{X}$ is:\n",
    "$$ \n",
    "\\Large E(f|\\mathbf{X}) = \\frac{1}{N} \\sum_{t=1}^{N} [y^t - f(\\mathbf{x}^t)]^2   \n",
    "$$\n",
    "* ### Because $y$ and $f(\\mathbf{x})$ are numeric quantities, for example, $\\in \\mathbb{R}$, there is an ordering defined on their values and we can define a distance between values, as the square of the difference, which gives us more information than equal/not equal, as used in classification.\n",
    "* ### The square of the difference is one error (loss) function that can be used; another is the absolute value of the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c181aee3-1835-4469-9ab5-c34d52f9ca11",
   "metadata": {},
   "source": [
    "## <span style='color:Blue'>Example: Building a sample regression model:</span> [[ref]](https://medium.com/@neslihannavsar/building-a-linear-regression-model-with-scikit-learn-part-1-eed4c04f53f9)\n",
    "* ### Linear Regression fits a linear model with coefficients to minimize the residual sum of squares between the actual value y in the dataset, and the predicted value that using linear approximation.\n",
    "* ### You can download the dataset from [here](https://open.canada.ca/data/en/dataset/98f1a129-f628-4ce4-b24d-6f16bf24dd64?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2022-01-01), which contains model-specific fuel consumption ratings, and estimated carbon dioxide emissions for new light-duty vehicles for retail sale in Canada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d81a07-ea5c-474c-a973-3f49c5f4252c",
   "metadata": {},
   "source": [
    "#### Install and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ee63f-e645-4d5a-b3ef-bba4c3994195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 500)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ce5fbc-4c24-4184-b61b-f078bcca0997",
   "metadata": {},
   "source": [
    "#### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d3c8e2-4199-454a-8638-ce23e1207d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"data/Fuel_Consumption_Ratings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770af7ec-8a0c-4b9f-b6cf-14d3a9736b3c",
   "metadata": {},
   "source": [
    "#### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ae1a4d-6cc9-4d0e-a2f4-753f914ddfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e4320-7cf1-4005-a631-d5c589e0b11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T\n",
    "df.isnull().any()\n",
    "df_ = df[['Fuel Consumption (L/100km)', 'CO2 emissions (g/km)']]\n",
    "df_.head()\n",
    "\n",
    "df_.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f5c280-3c4d-469c-b0d8-4ffb8bc60d8a",
   "metadata": {},
   "source": [
    "#### Splitting train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e1a827-207e-4835-9311-242c2fe3c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_[['Fuel Consumption (L/100km)']]\n",
    "y = df_[['CO2 emissions (g/km)']]\n",
    "X = df_[['Fuel Consumption (L/100km)']]\n",
    "y = df_[['CO2 emissions (g/km)']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "[X_train,y_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6179eeba-10f4-4f12-9fe9-9e1d0468cab3",
   "metadata": {},
   "source": [
    "#### Scatter plot between fuel consumption and CO2 emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3763580-2e45-4c6d-bbc5-121b22425886",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train, color='blue')\n",
    "plt.xlabel(\"Fuel Consumption (L/100km)\")\n",
    "plt.ylabel(\"CO2 Emissions (g/km)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1699b6-4f9e-4565-aec2-8f600e5d6638",
   "metadata": {},
   "source": [
    "#### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bff316-4310-48bc-baa8-75b0f6955a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498efe9a-0abd-4dd1-8919-a7d9de13fa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Intercept: ',reg_model.intercept_)   \n",
    "print ('Coefficients: ', reg_model.coef_[0])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83061da6-2742-4d26-ba09-d2b370905cc6",
   "metadata": {},
   "source": [
    "#### Plot the computed linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca007879-2398-4618-b8ff-ca41c18b98c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.regplot(x=X_train, y=y_train, scatter_kws={'color': 'b', 's':9},\n",
    "                 ci=False, color='r')  #güven aralığı false, yani ekleme\n",
    "g.set_title(f'Model Equation: CO2 Emissions = {round(reg_model.intercept_[0], 2)} + Fuel*{round(reg_model.coef_[0][0], 2)}')\n",
    "g.set_ylabel('CO2 Emissions')\n",
    "g.set_xlabel('Fuel Consumption')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5def08-3c7a-4cf3-bcfe-8728d406e733",
   "metadata": {},
   "source": [
    "#### Prediction & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e420f0-37f2-41e7-885b-e806881cf9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model.intercept_[0] + reg_model.coef_[0][0]*26.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e00838-6138-49bb-8032-b7483b52ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score: {:.2f}\".format(reg_model.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(reg_model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ca24d-e932-4ab6-952b-1b7227f18dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
