{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92352685-919b-40b2-8912-5ad5531c1167",
   "metadata": {},
   "source": [
    "# <span style='color:Red'>Bayesian decision theory:</span>[Ref: Chapter 3 of [[1]](https://erp.metbhujbalknowledgecity.ac.in/StudyMaterial/01VM092015008350131.pdf)]\n",
    "***\n",
    "   * #### Data comes from a process that is not completely known. This lack of knowledge is indicated by modeling the process as a random process.\n",
    "   * #### Maybe the process is actually deterministic, but because we do not have access to complete knowledge about it, we model it as random and use probability theory to analyze it.\n",
    "     <br>\n",
    "   * #### Tossing a coin is a random process because we cannot predict at any toss whether the outcome will be heads or tails—that is why we toss coins.\n",
    "   * #### We can only talk about the probability that the outcome of the next toss will be heads or tails.\n",
    "   * #### It may be argued that if we have access to extra knowledge such as the exact composition of the coin, its initial position, the force and its direction that is applied to the coin when tossing it, where and how it is caught, and so forth, the exact outcome of the toss can be predicted.\n",
    "   * #### The extra pieces of knowledge that we do not have access to are named the _unobservable variables_. In the coin tossing example, the only _observable variable_ is the outcome of the toss.\n",
    "     <br>\n",
    "   * #### Denoting the unobservables by $z$ and the observable as $\\chi$, in reality we have $\\chi = f(z)$, where $f(.)$ is the deterministic function that defines the outcome from the unobserved pieces of knowledge.\n",
    "   * #### However, we can not model the process this way due to lack of the \"unobserved pieces of knowledge\".\n",
    "   * #### Therefore, we define the outcome $C$ as a random variable drawn from a probability distribution $P(C = c)$ that specifies the process.\n",
    "     <br>\n",
    "   * #### Let the random variable $C$ denotes the outcome of tossing a coin, and takes either of the two values $\\{0,1\\}$. Here $0$ is when the outcome of tossing a coin is \"No Heads (or tails)\", and $1$ is for \"Heads\".\n",
    "   * #### Since the outcome of each toss is either $1$ or $0$, this can be modeled through Bernoulli distribution, where the parameter of the distribution $p_0$ is the probability that the outcome is heads:\n",
    "     $$\n",
    "      \\Large P(C=1)\\ =\\ p_0,\n",
    "      $$\n",
    "     $$\n",
    "      \\Large P(C=0)\\ =\\ 1-p_0\n",
    "     $$\n",
    "   * #### Assume that we are asked to predict the outcome of the next toss. If we know $p_0$, our prediction will be heads if $p_0$ > 0.5 and tails otherwise. This is because if we choose the more probable case.\n",
    "   *  #### The toss coining is a simple experiment that we are fully aware of. For a fair coin, we know that $p_0=0.5$, which gives every new toss an equal chance for head and tail.\n",
    "   *  #### However, in general, we do not know $P(C)$ and we have to estimate it from a given sample.\n",
    "      <br>\n",
    "   *  #### Let's reconsider the case of coin tossing if we have no prior knowledge about the coin. That is, we dont know what is $p_0$, and we want to estimate it from the previous outcomes.\n",
    "   *  #### Let the sample $\\chi$ includes the previous 9 outcomes, $\\chi=\\{1,1,1,0,1,0,0,1,1\\}$, which respectively belongs to $\\{heads, heads, heads, tails, heads, tails, tails, heads, heads\\}$.\n",
    "   *  #### Then using $\\chi$, we can estimate $p_0$, which is the parameter that uniquely specifies the distribution.\n",
    "   *  #### Our estimate of $p_0$ is:\n",
    "         $$\n",
    "         \\Large    \\hat{p}_0=\\frac{\\#\\{tosses\\ with\\ outcome\\ heads\\}}{\\#\\{tosses\\}}\n",
    "         $$\n",
    "         $$\n",
    "         \\Large    \\hat{p}_0=\\frac{6}{9}\n",
    "         $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7790fb8b-d2da-4837-91ba-412f3db6adb3",
   "metadata": {},
   "source": [
    " * ## <span style='color:Blue'>Classification rule:</span>\n",
    "     * #### Consider the example of credit scoring, which is intended to identify the \"High-risk\" customers.\n",
    "     * #### In a bank, according to their past transactions, some customers are 'low-risk' in that they paid back their loans and the bank profited from them and other customers are 'high-risk' in that they defaulted.\n",
    "     * #### Analyzing this data, we would like to learn the class “high-risk customer” so that in the future, when there is a new application for a loan, we can check whether that new person obeys the class description or not and thus accept or reject the application.\n",
    "       <br>\n",
    "     * #### Using our knowledge of the applications, let us say that we decide that there are two pieces of information that are observable. Customer’s yearly 'income' and 'savings'.\n",
    "     * #### We observe them because we have reason to believe that they give us an idea about the credibility of a customer.\n",
    "     * #### Let us say, for example, we observe customer’s yearly 'income' and 'savings', which we represent by two random variables $X1$ and $X2$.\n",
    "       <br>\n",
    "     * #### It may again be claimed that if we had access to other pieces of knowledge such as the state of economy in full detail and full knowledge about the customer, his or her intention, moral codes, and so forth, whether someone is a low-risk or high-risk customer could have been deterministically calculated. But these are 'non-observables'.\n",
    "     * #### With what we can observe, the credibility of a customer is denoted by a Bernoulli random variable $C$ conditioned on the observables $\\mathbf{X}=[X_1,X_2]^T$. Here $C=1$ indicates a high-risk customer and $C=0$ indicates a low-risk customer.\n",
    "     * #### Thus, if we know $P(C|X_1,X_2)$, when a new application arrives with $X_1=x_1$ and $X_2=x_2$, we can\n",
    "       $$\n",
    "       \\Large\n",
    "       choose \\begin{cases}\n",
    "       C=1 & \\mathrm{if}\\ P(C=1|x_1,x_2) > 0.5\\\\\n",
    "      C=0 & \\mathrm{otherwise}\n",
    "        \\end{cases}    \n",
    "       $$\n",
    "   <br>\n",
    "    * #### This example is similar to the coin tossing example except that here, the Bernoulli random variable $C$ is conditioned on two observable variables.\n",
    "    * #### Let us denote by $\\boldsymbol{\\chi}$ the vector of observed variables, $\\boldsymbol{\\chi}\\ =\\ [\\chi_1,\\chi_2]^T$. The problem then is to be able to calculate $P(C|\\boldsymbol{\\chi})$.\n",
    "    * #### Using _Bayes' Rule_, it can be written as:\n",
    "      $$\n",
    "          \\Large P(C|\\boldsymbol{\\chi})=\\frac{P(C)p(\\boldsymbol{\\chi}|C)}{p(\\boldsymbol{\\chi})}\n",
    "      $$\n",
    "    * #### Prior probability $P(C)$:\n",
    "       * #### $P(C)$ in the above equation is the _prior probability_. $P(C=1)$ is called the _prior probability_ that $C$ takes the value 1, which corresponds to the probability that a customer is 'high risk', regardless of the $\\boldsymbol{\\chi}$ value.\n",
    "       * #### It is called the _prior probability_ because it is the knowledge we have as to the value of $C$ before looking at the observables $\\boldsymbol{\\chi}$, satisfying\n",
    "         $$\n",
    "         \\Large P(C=0)\\ +\\ P(C=1)\\ =\\ 1\n",
    "         $$\n",
    "    * #### Class likelihood $p(\\boldsymbol{\\chi}|C)$:\n",
    "        * #### $p(\\boldsymbol{\\chi}|C)$ is called the _class likelihood_ and is the conditional probability that an event belonging to $C$ has the associated observation value $\\boldsymbol{\\chi}$.\n",
    "        * #### In our case, $p(\\chi_1,\\chi_2|C=1)$ is the probability that a high-risk customer has his or her $X_1=\\chi_1$ and $X_2=\\chi_2$.\n",
    "        * #### Note that, we call $p(\\boldsymbol{\\chi}|C)$ either the \"likelihood of $C$ (given $\\boldsymbol{\\chi}$)\" or the \"probability of $\\boldsymbol{\\chi}$ given $C$\" but never the likelihood of $\\boldsymbol{\\chi}$.\n",
    "    * #### Evidence $p(\\boldsymbol{\\chi})$:\n",
    "        * #### $p(\\boldsymbol{\\chi})$, the evidence, is the marginal probability that an observation x is seen, regardless of whether it is a positive or negative example.\n",
    "          $$\n",
    "           \\Large p(\\boldsymbol{\\chi})=\\sum_{C} p(\\boldsymbol{\\chi},C)=p(\\boldsymbol{\\chi}|C=1) p(C=1)+p(\\boldsymbol{\\chi}|C=0)p(C=0)\n",
    "          $$\n",
    "    * #### Posterior probability $P(C|\\boldsymbol{\\chi})$:\n",
    "        * #### Combining the prior and what the data tells us using Bayes’ rule, we calculate the posterior probability of the concept, $P(C|\\boldsymbol{\\chi})$, after having seen the observation, $\\boldsymbol{\\chi}$.\n",
    "          $$\n",
    "            \\Large posterior\\ =\\ \\frac{prior\\ \\mathbf{x}\\ likelihood}{evidence}\n",
    "          $$\n",
    "        * #### Because of normalization by the evidence, the posteriors sum up to 1:\n",
    "          $$\n",
    "          P(C=0|\\boldsymbol{\\chi})+P(C=1|\\boldsymbol{\\chi})=1\n",
    "          $$\n",
    "          <br>\n",
    "    * #### Once we have posteriors, we decide the class following:\n",
    "      $$\n",
    "       \\Large\n",
    "       choose \\begin{cases}\n",
    "       C=1 & \\mathrm{if}\\ P(C=1|x_1,x_2) > P(C=0|x_1,x_2)\\\\\n",
    "      C=0 & \\mathrm{otherwise}\n",
    "        \\end{cases}    \n",
    "       $$\n",
    "    <br>\n",
    "* ### In the general case, we have $K$ mutually exclusive and exhaustive classes; $C_i, i = 1, . . . , K$; for example, in optical digit recognition, the input is a bitmap image and there are ten classes.\n",
    "* ### We have the prior probabilities satisfying\n",
    "  $$\n",
    "       \\Large P(C_i)\\ge 0\\ \\mathrm{and}\\ \\sum_{i=1}^{K} P(C_i)=1\n",
    "       $$\n",
    "* ### $p(\\boldsymbol{\\chi}|C_i)$ is the probability of seeing $\\boldsymbol{\\chi}$ as the input when it is known to belong to class $C_i$ . The posterior probability of class $C_i$ can be calculated as:\n",
    "  $$\n",
    "     \\Large P(C_i|\\boldsymbol{\\chi})=\\frac{P(C_i)p(\\boldsymbol{\\chi}|C_i)}{p(\\boldsymbol{\\chi})} = \\frac{P(C_i)p(\\boldsymbol{\\chi}|C_i)}{\\sum_{k=1}^{K}p(\\boldsymbol{\\chi}|C_k)P(C_k)} \\tag 1\n",
    "  $$\n",
    "* ### Then, the _Bayes' classifier_ chooses the class with the highest posterior probability:\n",
    "  $$\n",
    "     \\Large \\mathrm{choose}\\ C_i\\ if\\ P(C_i|\\boldsymbol{\\chi})\\ =\\ \\max_{k} P(C_k|\\boldsymbol{\\chi})\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e59bfd4-58c2-41dd-97b3-f75eb0a301c3",
   "metadata": {},
   "source": [
    "* ## <span style='color:Blue'>Losses and Risks:</span>\n",
    "    * #### It may be the case that decisions are not equally good or costly.\n",
    "    * #### A financial institution when making a decision for a loan applicant should take into account the potential gain and loss as well.\n",
    "    * #### An accepted low-risk applicant increases profit, while a rejected high-risk applicant decreases loss.\n",
    "    * #### The loss for a high-risk applicant erroneously accepted may be different from the potential gain for an erroneously rejected low-risk applicant.\n",
    "    * #### The situation is much more critical and far from symmetry in other domains like medical diagnosis or earthquake prediction.\n",
    "      <br>\n",
    "    * ####  Let's define, $\\alpha_i$ as the decision to assign the input to class $C_i$.\n",
    "    * ####  $\\lambda_{ik}$ as the loss incurred for taking decision $\\alpha_i$ when the input actually belongs to $C_k$.\n",
    "    * #### In the case of $0/1$ loss method\n",
    "\n",
    "      $$\n",
    "          \\Large\n",
    "       \\lambda_{ik}= \\begin{cases}\n",
    "          0 & \\mathrm{if}\\ i=k \n",
    "          \\\\ 1 & \\mathrm{if}\\ i\\ne k\n",
    "        \\end{cases}    \n",
    "       $$\n",
    "        * ##### All correct decisions have no loss and all errrors are equally costly.\n",
    "   \n",
    "    * #### Then the expected risk for taking decision $\\alpha_i$ is\n",
    "      $$\n",
    "         \\Large R(\\alpha_i|\\boldsymbol{\\chi})=\\sum_{k=1}^{K}\\ \\lambda_{ik}\\ P(C_k|\\boldsymbol{\\chi})\n",
    "      $$\n",
    "    * #### Then, we choose the decision with minimum risk:\n",
    "      $$\n",
    "          \\Large \\mathrm{choose}\\ \\alpha_i\\ \\mathrm{if}\\ R(\\alpha_i|\\boldsymbol{\\chi})= \\min_{k}R(\\alpha_k|\\boldsymbol{\\chi})\n",
    "      $$\n",
    "  <br>\n",
    "    * #### Consider a three class $(K=3)$ classification example, with classes $\\{C_1, C_2, C_3\\}$. The risk involved in assigning the input data $\\boldsymbol{\\chi}$ to class $C_1$ is:\n",
    "      $$\n",
    "         \\Large R(\\alpha_1|\\boldsymbol{\\chi})=\\ \\lambda_{11}\\ P(C_1|\\boldsymbol{\\chi})\\ +\\ \\lambda_{12}\\ P(C_2|\\boldsymbol{\\chi})\\ +\\ \\lambda_{13}\\ P(C_3|\\boldsymbol{\\chi})\n",
    "      $$\n",
    "        * ##### Here, $\\lambda_{11}$ is the loss incurred in assigning input data $\\boldsymbol{\\chi}$ to class $C_1$, when it actually belongs to $C_1$. i.e. [$\\lambda_{11}$=0]\n",
    "        * ##### And, $\\lambda_{12}$ is the loss incurred in assigning input data $\\boldsymbol{\\chi}$ to class $C_2$, when it actually belongs to $C_1$. i.e. [$\\lambda_{12}$=1]\n",
    "        * ##### And, $\\lambda_{13}$ is the loss incurred in assigning input data $\\boldsymbol{\\chi}$ to class $C_3$, when it actually belongs to $C_1$. i.e. [$\\lambda_{13}$=1]\n",
    "    * #### Therefore,\n",
    "      $$\n",
    "         \\Large R(\\alpha_1|\\boldsymbol{\\chi})=\\ 0*\\ P(C_1|\\boldsymbol{\\chi})\\ +\\ 1*\\ P(C_2|\\boldsymbol{\\chi})\\ +\\ 1*\\ P(C_3|\\boldsymbol{\\chi})\n",
    "      $$\n",
    "      $$\n",
    "         \\Large R(\\alpha_1|\\boldsymbol{\\chi})=\\ P(C_2|\\boldsymbol{\\chi}) +\\ P(C_3|\\boldsymbol{\\chi}) = 1\\ -P(C_1|\\boldsymbol{\\chi}) \n",
    "      $$\n",
    "       * ##### If the input data $\\boldsymbol{\\chi}$ belongs to class $C_1$, then both the $P(C_2|\\boldsymbol{\\chi})$ and $P(C_2|\\boldsymbol{\\chi})$ are expected to be LOW, therefore, the risk of assigning $\\boldsymbol{\\chi}$ to class $C_1$, i.e., $R(\\alpha_1|\\boldsymbol{\\chi})$ will be LOW.\n",
    "      * #####  On the other hand, if the input data $\\boldsymbol{\\chi}$ belongs to class $C_2$ or class $C_3$, their corresponding probabilities (i.e., $P(C_2|\\boldsymbol{\\chi})$ and $P(C_3|\\boldsymbol{\\chi})$, respectively) are expected to be HIGH, therefore, the risk of assigning $\\boldsymbol{\\chi}$ to class $C_1$, i.e., $R(\\alpha_1|\\boldsymbol{\\chi})$ will be HIGH.\n",
    "      * ##### This enables us to see $R(\\alpha_1|\\boldsymbol{\\chi})$ as a discriminant function that can discriminate class-$C_1$ objects from class-$C_2$ objects.\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5d797b-5f40-4228-bec3-0205dafe2115",
   "metadata": {},
   "source": [
    "* ## <span style='color:Blue'>Discriminant Functions:</span>\n",
    "    * #### Let $g_i(\\chi)$ be the discriminant function that can discriminate class-$C_i$ objects from non-$C_i$ objects.\n",
    "    * #### In terms of the risk function $R(\\alpha_i|\\boldsymbol{\\chi})$, the discriminant function $g_i(\\chi)$ can be defined as:\n",
    "      $$\n",
    "        \\Large g_i(\\chi)=-R(\\alpha_i|\\boldsymbol{\\chi})\n",
    "      $$\n",
    "      * ##### This is due to the fact that minimum conditional risk corresponds to maximum discriminant function. \n",
    "    * #### Alternately, $g_i(\\chi)$ can itself be proportional to $P(C_i|\\boldsymbol{\\chi})$, that allow us to write\n",
    "      $$\n",
    "      \\Large    g_i(\\chi)=P(C_i|\\boldsymbol{\\chi})\n",
    "      $$\n",
    "     * #### From Bayes' rule $P(C_1|\\boldsymbol{\\chi})=\\frac{P(C_i)p(\\boldsymbol{\\chi}|C_i)}{p(\\boldsymbol{\\chi})}$. However the denomenator is a common normalizing term, which can be ignored from $g_i(\\chi)$:\n",
    "       $$\n",
    "      \\Large    g_i(\\chi)=P(C_i|\\boldsymbol{\\chi})=P(C_i)p(\\boldsymbol{\\chi}|C_i) \\tag 2\n",
    "      $$\n",
    "    * #### This enables us to see a K-class classification task as implementing a set of discriminant functions, $g_i(\\chi)$, $i = 1, . . . , K$, such that we choose $C_i$ if\n",
    "      $$\n",
    "      \\Large g_i(\\chi)= \\max_{k} g_k(\\chi)\n",
    "      $$\n",
    "    * #### For a 3-class classification problem, we choose three discriminant functions {$g_1(\\chi), g_2(\\chi), g_3(\\chi)$}, such that the input data point $x \\in \\chi$ classifies as:\n",
    "  $$        \n",
    "       \\Large       x \\in \n",
    "              \\begin{cases}\n",
    "              C_1 & \\mathrm{if}\\ max\\ \\{g_1(x), g_2(x), g_3(x)\\} = g_1(x) \\\\\n",
    "              C_2 & \\mathrm{if}\\ max\\ \\{g_1(x), g_2(x), g_3(x)\\} = g_2(x) \\\\\n",
    "              C_3 & \\mathrm{if}\\ max\\ \\{g_1(x), g_2(x), g_3(x)\\} = g_3(x) \\\\\n",
    "            \\end{cases}\n",
    "          $$\n",
    "     * #### With this, the existing feature space can be seen as divided into $K$ _decision regions_ $\\mathcal{R}_1,\\mathcal{R}_2,....,\\mathcal{R}_K$, where each region will have a specific class of data points:\n",
    "       $$\n",
    "       \\Large \\mathcal{R}_i= \\{\\chi|g_i(\\chi)=\\max_{k} g_k(\\chi)\\}\n",
    "       $$\n",
    "       #### is the region that has only class-$C_i$ data points\n",
    "     * #### The regions are separated by _decision boundaries_, surfaces in features space where ties occur among the largest discriminant functions\n",
    "    &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;  <img src=\"images/Decision_Region.png\" width=\"600\" height=\"300\">\n",
    "    * #### However, for the binary classification problem, a single discriminant function can be defined as:\n",
    "      $$\n",
    "       \\Large g(\\chi)= g_1(\\chi)-g_2(\\chi)\n",
    "       $$\n",
    "      #### and we \n",
    "      $$\n",
    "      choose\n",
    "      \\Large\n",
    "          \\begin{cases}\n",
    "              C_1 & \\mathrm{if}\\ g(\\chi) > 0 \\\\\n",
    "              C_2 & \\mathrm{if}\\ otherwise \\\\\n",
    "            \\end{cases}\n",
    "      $$\n",
    "   * #### An example is a two-class learning problem where the positive examples can be taken as $C_1$ and the negative examples as $C_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe6154-3285-47ac-acd6-7398ccce20b2",
   "metadata": {},
   "source": [
    "* ## <span style='color:Blue'>Maximum Likelihood Estimation:</span> [Ref: Section 4.2 from  [[1]](https://erp.metbhujbalknowledgecity.ac.in/StudyMaterial/01VM092015008350131.pdf) and the [weblink](https://www.probabilitycourse.com/chapter8/8_2_3_max_likelihood_estimation.php)]\n",
    "    * ### Let us say we have an independent and identically distributed (iid) sample $\\chi = \\{x^t\\}_t^N$.\n",
    "    * ### We assume that $x^t$ are instances drawn from some known probability density family, $p(\\chi|θ)$, defined up to parameters, $\\theta$:\n",
    "    $$\n",
    "        \\Large \\mathbf{x}^t \\sim p(\\chi|θ)\n",
    "    $$\n",
    "    * ### We want to find $\\theta$ that makes sampling $x^t$ from $p(x|\\theta)$ as likely as possible.Refer example 8.7 from [here](https://www.probabilitycourse.com/chapter8/8_2_3_max_likelihood_estimation.php).\n",
    "    * ### Because $\\mathbf{x}^t$ are independent, the likelihood of parameter $\\theta$ given sample $\\chi$ is the product of the likelihoods of the individual points:\n",
    "    $$\n",
    "        \\Large \\mathcal{l}(\\theta|\\chi) \\equiv p(\\chi|θ) = \\prod_{t=1}^{N} p(x^t|\\theta)\n",
    "    $$\n",
    "    * ### In _maximum likelihood estimation_, we are interested in finding $\\theta$ that estimation makes $\\chi$ the most likely to be drawn.\n",
    "    * ### We thus search for $\\theta$ that maximizes the likelihood, which we denote by $\\mathcal{l}(θ|\\chi)$.\n",
    "    * ### We can maximize the log of the likelihood without changing the value where it takes its maximum.\n",
    "    * ### $log(·)$ converts the product into a sum and leads to further computational simplification when certain densities are assumed, for example, containing exponents.\n",
    "    * ### The log likelihood is defined as:\n",
    "      $$\n",
    "         \\Large \\mathcal{L}(\\theta|\\chi)\\equiv log\\ \\mathcal{l}(\\theta|\\chi)=\\sum_{t=1}^{N} log\\ p(x^t|\\theta)\n",
    "      $$\n",
    "    * ### <span style='color:Green'> For Bernoulli distribution:\n",
    "        * ### In a Bernoulli distribution, there are two outcomes: An event occurs or it does not; for example, an instance is a positive example of the class, or it is not.\n",
    "        * ### The event occurs and the Bernoulli random variable $X$ takes the value 1 with probability $\\theta$, and the nonoccurrence of the event has probability $1 − \\theta$ and this is denoted by $X$ taking the value $0$.\n",
    "        * ### The probability density function for X is:\n",
    "          $$\n",
    "              \\Large p(x|\\theta)=\\theta^{x} (1-\\theta)^{1-x},\\ x \\in \\{0,1\\}\n",
    "          $$\n",
    "       \n",
    "        * ### The expected value and variance can be calculated as:\n",
    "          $$\n",
    "             \\Large E[X]=\\sum_{x}x.p(x)\\ =\\ 1.\\theta\\ +\\ 0.(1-\\theta)\\ =\\ \\theta\n",
    "          $$\n",
    "          $$\n",
    "              \\Large Var(X)=\\sum_{x} (x-E[X])^2\\ p(x)\\ =\\ \\theta(1-\\theta)\n",
    "          $$\n",
    "        * ###  It can be observed that $p$ is the only parameter in Bernoulli distribution.\n",
    "        * ### Therefore, for a given iid sample $\\chi=\\{x^t\\}_{t=1}^{N}$, where $x^t \\in \\{0,1\\}$, we want to calculate its estimator $\\hat{\\theta}$.\n",
    "        * ### The log likelihood is:\n",
    "           $$\n",
    "         \\Large \\mathcal{L}(\\theta|\\chi) =\\sum_{t=1}^{N} log\\ p(x^t|\\theta) = \\sum_{t=1}^{N} log\\ \\theta^{(x^t)} (1-\\theta)^{1-(x^t)}\n",
    "          $$\n",
    "          $$\n",
    "         \\Large \\mathcal{L}(\\theta|\\chi) = \\sum_{t=1}^{N} x^t\\ log\\ \\theta\\ +\\sum_{t=1}^{N} (1-x^t)\\ log\\ (1-\\theta)\n",
    "          $$\n",
    "           $$\n",
    "         \\Large \\mathcal{L}(\\theta|\\chi) = \\sum_{t=1}^{N} x^t\\ log\\ \\theta\\ + (N-\\sum_{t=1}^{N} x^t)\\ log\\ (1-\\theta)\n",
    "          $$\n",
    "        * ### The $\\hat{\\theta}$ that maximizes the log likelihood can be found by solving for $\\frac{\\partial \\mathcal{L}}{\\partial \\theta}=0$\n",
    "          $$\n",
    "          \\Large \\hat{\\theta}=\\frac{\\sum_{t} x^t}{N}\n",
    "          $$\n",
    "        * ### The estimate for $\\theta$ is the ratio of the number of occurrences of the event to the number of experiments.\n",
    "        * ### Remembering that if $X$ is Bernoulli with parameter $\\theta$, $E[X] = \\theta$, and, as expected, the maximum likelihood estimator of the mean is the sample average.\n",
    "          <br>\n",
    "    * ### <span style='color:Green'> For Gaussian (Normal) distribution:\n",
    "        * ### Let $X$ be the Gaussian (normal) distributed random variable with mean $E[X] = \\theta_1$ and variance $Var(X) = \\theta_2$, denoted as $N(\\theta_1,\\theta_2)$, and its density function is\n",
    "          $$\n",
    "           \\Large p(x|\\theta_1,\\theta_2)= \\frac{1}{\\sqrt{2\\pi\\theta_2}} e^{\\frac{-(x-\\theta_1)^2}{2\\theta_2}}\n",
    "          $$\n",
    "        * ### Given a sample $\\chi=\\{x^t\\}_{t=1}^{N}$ with $x^t \\sim\\ N(\\theta_1,\\theta_2)$, the log likelihood is:\n",
    "          $$\n",
    "            \\Large  \\mathcal{L}(\\theta_1,\\theta_2|\\chi)=-\\frac{N}{2}log(2\\pi)-\\frac{N}{2}log(\\theta_2)-\\frac{1}{2\\theta_2}\\sum_{t=1}^{N}(x_i-\\theta_1)^2\n",
    "          $$\n",
    "        * ### Take the partial derivatives with respect to $\\theta_1$ and $\\theta_2$, and then equate to zero to get the maximum likelihoods:\n",
    "          $$\n",
    "          \\Large \\frac{\\partial \\mathcal{L}}{\\partial \\theta_1}=\\frac{1}{\\theta_2}\\sum_{t=1}^{N}(x_i-\\theta_1)=0\n",
    "          $$\n",
    "          $$\n",
    "          \\Large \\frac{\\partial \\mathcal{L}}{\\partial \\theta_2}=\\frac{-N}{2\\theta_2}+\\frac{1}{2\\theta_2^2}\\sum_{t=1}^{N}(x_i-\\theta_1)^2=0\n",
    "          $$\n",
    "         * ### By solving the above equations, we obtain the maximum likelihood estimates for $\\theta_1$ and $\\theta_2$ are:\n",
    "           $$\n",
    "           \\Large \\hat{\\theta_1}=\\frac{\\sum_{t} x^t}{N}\n",
    "           $$\n",
    "           $$\n",
    "           \\Large \\hat{\\theta_2}=\\frac{\\sum_{t} (x^t-\\hat{\\theta_1})^2}{N}\n",
    "           $$\n",
    "           \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05408688-1160-4f6a-89be-5f617c05373a",
   "metadata": {},
   "source": [
    "* ## <span style='color:Blue'>Likelihood-based Classification:</span> [Ref: Section 4.5 from  [[1]](https://erp.metbhujbalknowledgecity.ac.in/StudyMaterial/01VM092015008350131.pdf)\n",
    "  * ### Using the Bayes' rule, the posterior probability of class $C_i$ is (from Equation 1):\n",
    "$$\n",
    "     \\Large P(C_i|\\boldsymbol{\\chi})=\\frac{P(C_i)p(\\boldsymbol{\\chi}|C_i)}{p(\\boldsymbol{\\chi})} = \\frac{P(C_i)p(\\boldsymbol{\\chi}|C_i)}{\\sum_{k=1}^{K}p(\\boldsymbol{\\chi}|C_k)P(C_k)}\n",
    "  $$\n",
    "  * ### From Equation (2), the discriminant function $g_i(\\chi)$ is:\n",
    "    $$\n",
    "      \\Large    g_i(\\chi)=P(C_i|\\boldsymbol{\\chi})=p(\\boldsymbol{\\chi}|C_i)P(C_i)\n",
    "      $$\n",
    "    ### or equivalently\n",
    "     $$\n",
    "      \\Large    g_i(\\chi)=log\\ p(\\boldsymbol{\\chi}|C_i)\\ + log\\ P(C_i)\n",
    "      $$\n",
    "   * ### Example:\n",
    "       * #### Assume an example of a car company selling $K$ different varieties of cars.\n",
    "       * #### For simplicity, assume that the sole feature on which the customers choice of buying the car depends on is \"Yearly Income\".\n",
    "       * #### This enables the structure of the dataset $\\chi=\\{ \\mathbf{x}^t,\\mathbf{y}^t\\}_{t=1}^N$is as follows:\n",
    "         $$\n",
    "      \\Large    \\chi = \\begin{bmatrix}   x_1^1 & y_1^1 & y_2^1 & . &. & y_K^1 \\\\\n",
    "                                  x_1^2 & y_1^2 & y_2^2 & . &. & y_K^2 \\\\\n",
    "                                  x_1^3 & y_1^3 & y_2^3 & . &. & y_K^3 \\\\\n",
    "                                  . & . & . & . &. & . \\\\\n",
    "                                  . & . & . & . &. & . \\\\\n",
    "                                  x_1^N & y_1^N & y_2^N & . &. & y_K^N \\\\\n",
    "      \\end{bmatrix} \\in \\mathbb{R}^{[N\\ \\mathrm{x}\\ (K+1)]}\n",
    "$$\n",
    "      * #### From this notation, for the $t^{th}$ data instance, the input is $x_1^t \\in \\mathbb{R}$, which is a scalar due to the consideration of only one feature.\n",
    "      * #### Whereas, for the $t^{th}$ data instance, the output is $\\mathbf{y}^t \\in \\mathbb{R}^{[1\\ \\mathrm{x}\\ K]}$, which is a $K$ dimensional vector of the form $\\mathbf{y}^t=[y_1^t, y_2^t, . ,. , y_K^t]$\n",
    "      * #### The output vector $\\mathbf{y}^t$ follows the below definition:\n",
    "        $$\n",
    "           \\Large y_i^t=\n",
    "            \\begin{cases}\n",
    "          1 & \\mathrm{if}\\ \\mathbf{x}^t \\in C_i\n",
    "          \\\\ 0 & \\mathrm{if}\\ \\mathbf{x}^t \\in C_k, k \\ne i\n",
    "        \\end{cases}    \n",
    "        $$\n",
    "        <br>\n",
    "     * #### Here, $P(C_i)$ which is needed for computing $g_i(\\chi)$ is the proportion of customers who buy car type $i$.\n",
    "     * #### If the yearly income distributions of the customers can be approximated with a Gaussian, then $p(x|C_i)$, the probability that a customer who bought car type $i$ has income $x$, can be taken $N(\\theta_1,\\theta_2)$, where $\\theta_1$ is the mean income of customers and $\\theta_2$ is their income variance.\n",
    "       $$\n",
    "           \\Large p(\\mathbf{x}|C_i)= \\frac{1}{\\sqrt{2\\pi\\theta_{2i}}} e^{\\frac{-(\\mathbf{x}-\\theta_{1i})^2}{2\\theta_{2i}}}\n",
    "          $$\n",
    "       $$\n",
    "        \\Large g_i(\\mathbf{x})=-\\frac{1}{2} log(2\\pi)-\\frac{1}{2}log(\\theta_{2i})-\\frac{(\\mathbf{x}-\\theta_{1i})^2}{2\\theta_{2i}}+log\\ P(C_i)\n",
    "       $$\n",
    "     * #### The unknown parameters $\\theta_{1i}$ and $\\theta_{2i}$ can be estimated by the maximum likelihood estimates:\n",
    "       $$\n",
    "           \\Large \\hat{\\theta_{1i}}=\\frac{\\sum_{c\\in C_i} x^c}{N_i}\\ \\mathrm{;\\ Mean\\ of\\ class}\\ C_i\\ \\mathrm{ data\\ points}\n",
    "           $$\n",
    "           $$\n",
    "           \\Large \\hat{\\theta_{2i}}=\\frac{\\sum_{c\\in C_i} (x^c-\\hat{\\theta_{1i}})^2}{N_i}\\ \\mathrm{;\\ Variance\\ of\\ class}\\ C_i\\ \\mathrm{ data\\ points}\n",
    "           $$\n",
    "       \n",
    "       ##### where $\\sum_{c\\in C_i} x^c$ is the sum of all class $C_i$ data points, and $N_i$ is the number of class $C_i$ datapoints.\n",
    "     * #### The priors $P(C_i)$ can be estimated as the ratio of number of class $C_i$ data points out of the total number of data points ($N$):\n",
    "     \n",
    "       $$\n",
    "           \\Large \\hat{P}(C_i)=\\frac{\\# class-C_i\\ data\\ points}{N}\n",
    "           $$\n",
    "     * #### After plugging the estimators, the $g_i(\\mathbf{x})$ becomes:\n",
    "       $$\n",
    "        \\Large g_i(\\mathbf{x})=-\\frac{1}{2} log(2\\pi)-\\frac{1}{2}log(\\hat{\\theta}_{2i})-\\frac{(\\mathbf{x}-\\hat{\\theta}_{1i})^2}{2\\hat{\\theta}_{2i}}+log\\ \\hat{P}(C_i) \n",
    "       $$\n",
    "     * #### The first term is a constant and can be dropped because it is common in all $g_i(\\mathbf{x})$.\n",
    "     * #### If the priors are equal, the last term can also be dropped.\n",
    "     * #### If we can further assume that variances are equal, we can write:\n",
    "       $$\n",
    "        \\Large g_i(\\mathbf{x})=-(\\mathbf{x}-\\hat{\\theta}_{1i})^2 \\tag 3\n",
    "       $$\n",
    "     * #### Thus, the data point $\\mathbf{x}$ can be assigned to the class with the nearest mean:\n",
    "       $$\n",
    "       \\Large \\mathrm{Choose}\\ C_i\\ \\mathrm{if}\\ |\\mathbf{x}-\\hat{\\theta}_{1i}|=\\min_k|\\mathbf{x}-\\hat{\\theta}_{1k}|\n",
    "       $$\n",
    "     * #### With two adjacent classes, the decision boundary will exactly be on the mid-way between the means. On the boundary the datapoint $\\mathbf{x}$ satisfies\n",
    "       $$\n",
    "        \\Large g_1(\\mathbf{x})= g_2(\\mathbf{x})\n",
    "       $$\n",
    "       $$\n",
    "        \\Large (\\mathbf{x}-\\hat{\\theta}_{1i})^2=(\\mathbf{x}-\\hat{\\theta}_{2i})^2\n",
    "       $$\n",
    "       $$\n",
    "        \\Large \\mathbf{x}=\\frac{\\hat{\\theta}_{1i}+\\hat{\\theta}_{2i}}{2}\n",
    "       $$\n",
    "     \n",
    " * ### This is the likelihood-based approach to classification where we use data to estimate the densities separately, calculate posterior densities using Bayes’ rule, and then get the discriminant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485a0f56-54f5-47f8-a61f-042c61a69545",
   "metadata": {},
   "source": [
    " * #### <span style='color:Blue'>Example:</span>\n",
    "     * #### Assume a three class classification problem, where the input data contains only one feature.\n",
    "     * #### Generating three groups of normally distributed data with different means and same variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b333da31-d911-441b-b8bb-7ca5004c59dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(60, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy import random\n",
    "import numpy as np\n",
    "x_C1 = random.normal(loc=1, scale=2, size=(20, 1))\n",
    "x_C2 = random.normal(loc=10, scale=2, size=(20, 1))\n",
    "x_C3 = random.normal(loc=20, scale=2, size=(20, 1))\n",
    "x=np.concatenate((x_C1,x_C2,x_C3), axis = 0)\n",
    "Dummy=np.ones((20, 1))\n",
    "plt.scatter(x_C1, Dummy, color='blue', label='Group 1')\n",
    "plt.scatter(x_C2, Dummy, color='red', label='Group 2')\n",
    "plt.scatter(x_C3, Dummy, color='green', label='Group 3')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf26175b-190e-4ec1-825a-6e29804143e8",
   "metadata": {},
   "source": [
    "### The estimates of mean $\\theta_{1i}$ for all the three classes using likelihood estimation is:\n",
    " $$\n",
    "           \\Large \\hat{\\theta_{1i}}=\\frac{\\sum_{c\\in C_i} x^c}{N_i}\\ \\mathrm{;\\ Mean\\ of\\ class}\\ C_i\\ \\mathrm{ data\\ points}\n",
    "           $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "237ed379-0958-460b-bd3d-92d77b82dac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.528639557434225\n",
      "10.351112369092302\n",
      "20.4767066034784\n"
     ]
    }
   ],
   "source": [
    "Theta_Hat_1_C1=np.divide(np.sum(x_C1),len(x_C1))\n",
    "print(Theta_Hat_1_C1)\n",
    "\n",
    "Theta_Hat_1_C2=np.divide(np.sum(x_C2),len(x_C2))\n",
    "print(Theta_Hat_1_C2)\n",
    "\n",
    "Theta_Hat_1_C3=np.divide(np.sum(x_C3),len(x_C3))\n",
    "print(Theta_Hat_1_C3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0766ada-a156-4e8f-b519-b00a09684250",
   "metadata": {},
   "source": [
    "### Prior probabilities for all the three classes using the below relation is:\n",
    "$$\n",
    "           \\Large \\hat{P}(C_i)=\\frac{\\# class-C_i\\ data\\ points}{N}\n",
    "           $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6701cae-76aa-4ddb-aee9-41cd346ddb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "0.3333333333333333\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "P_hat_C1=np.divide(len(x_C1),len(x))\n",
    "print(np.mean(P_hat_C1))\n",
    "\n",
    "P_hat_C2=np.divide(len(x_C2),len(x))\n",
    "print(np.mean(P_hat_C2))\n",
    "\n",
    "P_hat_C3=np.divide(len(x_C3),len(x))\n",
    "print(np.mean(P_hat_C3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092ee6cc-ae77-4613-94cc-3233de744d94",
   "metadata": {},
   "source": [
    "### It can be seen that the number of datapoints in each class are same, making the prior probability $\\hat{P}(C_i)$ same for all three classes. Additionally, the variance of data in all three classes is same. Therefore, the discriminant function $g_i(\\mathbf{x})$ will be same as the one in Equation (3):\n",
    "$$\n",
    "        \\Large g_i(\\mathbf{x})=-(\\mathbf{x}-\\hat{\\theta}_{1i})^2 \n",
    "       $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a66324-9c47-405a-aebc-056138287434",
   "metadata": {},
   "source": [
    "### Compute the discriminant functions {$g_1(x),\\ g_2(x),\\ g_3(x)$}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f0bc1d06-12c9-4e6d-817d-3e00de36defa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.74913036e+00, -3.43932976e+01, -2.55685669e+02],\n",
       "       [-1.16812315e+01, -1.49823830e+02, -5.00231188e+02],\n",
       "       [-1.20224891e+00, -5.96911022e+01, -3.18679477e+02],\n",
       "       [-7.62334313e-01, -9.40044731e+01, -3.92879360e+02],\n",
       "       [-2.09531648e+01, -1.80201108e+01, -2.06514269e+02],\n",
       "       [-6.95719935e-01, -9.32493652e+01, -3.91334065e+02],\n",
       "       [-1.77231150e-01, -8.54415734e+01, -3.75160305e+02],\n",
       "       [-1.95955472e-02, -7.53856103e+01, -3.53743984e+02],\n",
       "       [-3.39368730e+00, -4.87242598e+01, -2.92610813e+02],\n",
       "       [-1.31827086e+00, -9.94135141e+01, -4.03858332e+02],\n",
       "       [-6.99569291e+00, -1.31501493e+02, -4.66257833e+02],\n",
       "       [-2.40613375e+00, -1.07612501e+02, -4.20218791e+02],\n",
       "       [-5.60879067e-04, -7.74187042e+01, -3.58132316e+02],\n",
       "       [-1.57337557e+01, -1.63559868e+02, -5.25081042e+02],\n",
       "       [-6.97740559e+00, -1.31422166e+02, -4.66108452e+02],\n",
       "       [-7.82574232e+00, -3.63008658e+01, -2.60842340e+02],\n",
       "       [-9.38908488e-01, -6.16774608e+01, -3.23247826e+02],\n",
       "       [-1.95081377e+01, -1.94099375e+01, -2.11157671e+02],\n",
       "       [-1.77437984e+00, -1.03114506e+02, -4.11283492e+02],\n",
       "       [-8.93650926e-05, -7.76693127e+01, -3.58671090e+02],\n",
       "       [-7.99110139e+01, -1.36476696e-02, -1.00175497e+02],\n",
       "       [-4.61263662e+01, -4.12428421e+00, -1.47778732e+02],\n",
       "       [-5.29201695e+01, -2.39583271e+00, -1.36269248e+02],\n",
       "       [-8.06278229e+01, -2.45946691e-02, -9.93763226e+01],\n",
       "       [-7.69366735e+01, -2.61300173e-03, -1.03565462e+02],\n",
       "       [-8.97875240e+01, -4.26611565e-01, -8.97271034e+01],\n",
       "       [-6.75316928e+01, -3.65669239e-01, -1.15139341e+02],\n",
       "       [-6.80977199e+01, -3.25286061e-01, -1.14402978e+02],\n",
       "       [-6.35064919e+01, -7.28251887e-01, -1.20537804e+02],\n",
       "       [-8.29087769e+01, -8.00623298e-02, -9.68775889e+01],\n",
       "       [-8.86896206e+01, -3.54077216e-01, -9.08313901e+01],\n",
       "       [-3.69104373e+01, -7.54643069e+00, -1.65705658e+02],\n",
       "       [-1.31953078e+02, -7.10014794e+00, -5.56662816e+01],\n",
       "       [-4.66970797e+01, -3.95590880e+00, -1.46762101e+02],\n",
       "       [-1.38708561e+02, -8.73194858e+00, -5.14176071e+01],\n",
       "       [-7.71615896e+01, -1.46733634e-03, -1.03304864e+02],\n",
       "       [-8.44062625e+01, -1.33090189e-01, -9.52728058e+01],\n",
       "       [-9.38082282e+01, -7.44755157e-01, -8.57958005e+01],\n",
       "       [-5.98613542e+01, -1.17822525e+00, -1.25687756e+02],\n",
       "       [-1.36615801e+02, -8.21282919e+00, -5.27045628e+01],\n",
       "       [-5.06549803e+02, -1.87257015e+02, -1.26635931e+01],\n",
       "       [-3.73929043e+02, -1.10560434e+02, -1.51459456e-01],\n",
       "       [-3.57910095e+02, -1.01930006e+02, -8.73503216e-04],\n",
       "       [-3.85965260e+02, -1.17148710e+02, -4.87108414e-01],\n",
       "       [-3.80050910e+02, -1.13900584e+02, -2.99020212e-01],\n",
       "       [-4.33124475e+02, -1.43740196e+02, -3.47291450e+00],\n",
       "       [-5.10766523e+02, -1.89824232e+02, -1.33376684e+01],\n",
       "       [-3.07524820e+02, -7.59322760e+01, -1.99284452e+00],\n",
       "       [-3.81214856e+02, -1.14538185e+02, -3.32533493e-01],\n",
       "       [-2.61278677e+02, -5.38997388e+01, -7.75037809e+00],\n",
       "       [-3.54111661e+02, -9.99076621e+01, -1.69552153e-02],\n",
       "       [-3.83391195e+02, -1.15732520e+02, -3.99816880e-01],\n",
       "       [-3.52986486e+02, -9.93104282e+01, -2.56424075e-02],\n",
       "       [-4.30867580e+02, -1.42441294e+02, -3.27350494e+00],\n",
       "       [-2.73666230e+02, -5.96043844e+01, -5.78501904e+00],\n",
       "       [-4.18411009e+02, -1.35317954e+02, -2.27114043e+00],\n",
       "       [-3.72790175e+02, -1.09941562e+02, -1.29389799e-01],\n",
       "       [-2.14425106e+02, -3.38815806e+01, -1.85313511e+01],\n",
       "       [-2.01197678e+02, -2.87504427e+01, -2.26923642e+01],\n",
       "       [-3.74193724e+02, -1.10704377e+02, -1.56832258e-01]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_1=-np.square(x-Theta_Hat_1_C1)\n",
    "g_2=-np.square(x-Theta_Hat_1_C2)\n",
    "g_3=-np.square(x-Theta_Hat_1_C3)\n",
    "g=np.concatenate((g_1,g_2,g_3), axis = 1)\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3d9807-206a-4006-a210-9603149371dd",
   "metadata": {},
   "source": [
    "### Choose the class for $x$ as $max\\{g_1(x),\\ g_2(x),\\ g_3(x)\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "29277a9a-acde-413b-b5ac-6d02a07fc6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The datapoint x classified to class: [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"The datapoint x classified to class:\",np.argmax(g,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0f9e04-9c22-4a03-9df7-37fe0f1e2003",
   "metadata": {},
   "source": [
    "### Generate 10 test points belongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e753019-5017-42a7-8613-8bc2b4b62546",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test1 = random.normal(loc=1, scale=2, size=(5, 1))\n",
    "x_test2 = random.normal(loc=10, scale=2, size=(5, 1))\n",
    "x_test3 = random.normal(loc=20, scale=2, size=(5, 1))\n",
    "x_test=np.concatenate((x_test1,x_test2,x_test3), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f5f434-a03e-400b-b57a-f350d436af3e",
   "metadata": {},
   "source": [
    "### Compute the discriminant functions {$g_1(x_{test}),\\ g_2(x_{test}),\\ g_3(x_{test})$}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ca90b5f-f383-465f-a5be-ef0b4e8aaab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.41657651e+01, -1.52615556e+01, -1.96902590e+02],\n",
       "       [-1.40887890e+00, -5.83010247e+01, -3.15456851e+02],\n",
       "       [-1.68000191e+00, -5.66455520e+01, -3.11590215e+02],\n",
       "       [-3.24875995e+00, -1.12888595e+02, -4.30583197e+02],\n",
       "       [-2.10399410e-01, -8.61400417e+01, -3.76622341e+02],\n",
       "       [-9.56967916e+01, -9.21601700e-01, -8.40081014e+01],\n",
       "       [-7.25863526e+01, -9.16335479e-02, -1.08749536e+02],\n",
       "       [-4.54401958e+01, -4.33280239e+00, -1.49014089e+02],\n",
       "       [-9.74362136e+01, -1.09936387e+00, -8.23935392e+01],\n",
       "       [-8.35835016e+01, -1.02354241e-01, -9.61510806e+01],\n",
       "       [-3.73675818e+02, -1.10422760e+02, -1.46405114e-01],\n",
       "       [-2.45928991e+02, -4.70548046e+01, -1.06663887e+01],\n",
       "       [-3.43189165e+02, -9.41461254e+01, -1.78676644e-01],\n",
       "       [-4.28202893e+02, -1.40910932e+02, -3.04501396e+00],\n",
       "       [-3.89512738e+02, -1.19106758e+02, -6.20959806e-01]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_1=-np.square(x_test-Theta_Hat_1_C1)\n",
    "g_2=-np.square(x_test-Theta_Hat_1_C2)\n",
    "g_3=-np.square(x_test-Theta_Hat_1_C3)\n",
    "g=np.concatenate((g_1,g_2,g_3), axis = 1)\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1ab010-4aaf-406e-aef1-7c5ff1be1921",
   "metadata": {},
   "source": [
    "### Choose the class for $x_{test}$ as $max\\{g_1(x_{test}),\\ g_2(x_{test}),\\ g_3(x_{test})\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f9e5dc58-f833-4e49-af11-7157957b66b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test point x_test belongs to class: [1 0 0 0 0 1 1 1 1 1 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"The test point x_test belongs to class:\",np.argmax(g,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15655142-b6c4-4f8c-95eb-ceee9a760344",
   "metadata": {},
   "source": [
    "# <span style='color:Red'>Multivariate methods:</span>[Ref: Chapter 5 of [[1]](https://erp.metbhujbalknowledgecity.ac.in/StudyMaterial/01VM092015008350131.pdf)]\n",
    "***\n",
    " * ## <span style='color:Blue'>Multivariate Data:</span>\n",
    "     * #### In many applications, several measurements are made on each individual or event generating an observation vector.\n",
    "     * #### The sample may be viewed as a data matrix\n",
    "         $$\n",
    "      \\Large    \\boldsymbol X = \\begin{bmatrix}   x_1^1 & x_2^1 & x_3^1 & . &. & x_D^1 \\\\\n",
    "                                  x_1^2 & x_2^2 & x_3^2 & . &. & x_D^2 \\\\\n",
    "                                  x_1^3 & x_2^3 & x_3^3 & . &. & x_D^3 \\\\\n",
    "                                  . & . & . & . &. & . \\\\\n",
    "                                  . & . & . & . &. & . \\\\\n",
    "                                  x_1^N & x_2^N & x_3^N & . &. & x_D^N \\\\\n",
    "      \\end{bmatrix} \\in \\mathbb{R}^{[N\\ \\mathrm{x}\\ D]}\n",
    "$$\n",
    "    #### where, $D$ columns are the inputs, features, or attributes. And, $N$ rows are the independent and identically distributed (iid) observations, examples, or instances on $N$ individuals or events.\n",
    "    * #### For example, in deciding on a loan application, an observation vector is the information associated with a customer and is composed of age, marital status, yearly income, and so forth, and we have $N$ such past customers.\n",
    "    * #### These measurements may be of different scales, for example, age in years and yearly income in monetary units. Some like age may be numeric, and some like marital status may be discrete.\n",
    "    * #### Typically these variables are correlated. If they are not, there is no need for a multivariate analysis.\n",
    "    * #### Our aim may be simplification, that is, summarizing this large body of data by means of relatively few parameters. Or our aim may be exploratory, and we may be interested in generating hypotheses about data.\n",
    "    * #### In some applications, we are interested in predicting the value of one variable from the values of other variables.\n",
    "    * #### If the predicted variable is discrete, this is multivariate classification, and if it is numeric, this is a multivariate regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52e9586-5c56-49bf-8cf9-56ef9c7314a7",
   "metadata": {},
   "source": [
    " * ## <span style='color:Blue'>Parameter Estimation:</span>\n",
    "    * #### The _mean vector_ $\\Large \\boldsymbol \\mu$ is defined such that each of its elements is the mean of one column of $\\mathbf{X}$:\n",
    "      $$\n",
    "       \\Large \\boldsymbol \\mu\\ =\\ E[\\mathbf{X}]\\ =\\ [\\mu_1, \\mu_2, ......, \\mu_D]^T\n",
    "      $$\n",
    "      #### where $\\mu_1=E[\\mathbf{x}_1]=mean(x_1^1,x_1^2,x_1^3,......,x_1^N)$,   .........\n",
    "      #### $\\mu_D=E[\\mathbf{x}_D]=mean(x_D^1,x_D^2,x_D^3,......,x_D^N)$\n",
    "    * #### The variance of $\\mathbf{x}_i \\in \\mathbb{R}^{D\\ \\mathrm{x}\\ 1}$ is denoted as $\\sigma_i^2$, and the covariance of two variables $\\mathbf{x}_i$ and $\\mathbf{x}_j$ is defined as:\n",
    "      $$\n",
    "       \\Large \\sigma_{i,j} \\equiv Cov(\\mathbf{x}_i,\\mathbf{x}_j)=E[(\\mathbf{x}_i - \\mu_i)^T(\\mathbf{x}_j - \\mu_j)]=E[\\mathbf{x}_i^T\\mathbf{x}_j]-\\mu_i\\mu_j\n",
    "      $$\n",
    "    * #### The covariances for all possible combinations of $\\mathbf{x}_i$ and $\\mathbf{x}_j$, are generally represented by a covariance matrix $\\Sigma$, whose $(i,j)^{th}$ element is $\\sigma_{ij}$:\n",
    "       $$\n",
    "      \\Large    \\boldsymbol \\Sigma = \\begin{bmatrix}   \\sigma_1^2 & \\sigma_{12} & \\sigma_{13} & . &. & \\sigma_{1D} \\\\\n",
    "                                  \\sigma_{21} & \\sigma_2^2 & \\sigma_{23} & . &. & \\sigma_{2D} \\\\\n",
    "                                  \\sigma_{31} & \\sigma_{32} & \\sigma_3^3 & . &. & \\sigma_{3D} \\\\\n",
    "                                  . & . & . & . &. & . \\\\\n",
    "                                  . & . & . & . &. & . \\\\\n",
    "                                  \\sigma_{D1} & \\sigma_{D2} & \\sigma_{D3} & . &. & \\sigma_D^2 \\\\\n",
    "      \\end{bmatrix} \\in \\mathbb{R}^{[D\\ \\mathrm{x}\\ D]}\n",
    "$$\n",
    "        #### The matrix $\\Sigma$ is a symmetric matrix with $\\sigma_{ij}=\\sigma_{ji}$, and when $i=j$, $\\sigma_{ii}=\\sigma_i^2$. ALl the diagonal elements of $\\Sigma$ are the variances, and all the off-diagonal elements are covariances.\n",
    "     * #### In vector-matrix notation:\n",
    "   $$\n",
    "      \\Large    \\boldsymbol \\Sigma \\equiv Cov(\\mathbf{X})=E[(\\mathbf{X}-\\boldsymbol \\mu)^T(\\mathbf{X}-\\boldsymbol \\mu)] = E[\\mathbf{X}^T\\mathbf{X}]-\\boldsymbol \\mu \\boldsymbol \\mu^T\n",
    "$$\n",
    " $$\n",
    "     = E\\begin{bmatrix}  \\begin{pmatrix} x_1^1-\\mu_1 & x_2^1-\\mu_2 & x_3^1-\\mu_3 & . &. & x_D^1-\\mu_D \\\\\n",
    "                                  x_1^2-\\mu_1 & x_2^2-\\mu_2 & x_3^2-\\mu_3 & . &. & x_D^2-\\mu_D \\\\\n",
    "                                  x_1^3-\\mu_1 & x_2^3-\\mu_2 & x_3^3-\\mu_3 & . &. & x_D^3-\\mu_D \\\\\n",
    "                                  . & . & . & . &. & . \\\\\n",
    "                                  . & . & . & . &. & . \\\\\n",
    "                                  x_1^N-\\mu_1 & x_2^N-\\mu_2 & x_3^N-\\mu_3 & . &. & x_D^N-\\mu_D \\end{pmatrix}^T & \n",
    "                                  \\begin{pmatrix} x_1^1-\\mu_1 & x_2^1-\\mu_2 & x_3^1-\\mu_3 & . &. & x_D^1-\\mu_D \\\\\n",
    "                                  x_1^2-\\mu_1 & x_2^2-\\mu_2 & x_3^2-\\mu_3 & . &. & x_D^2-\\mu_D \\\\\n",
    "                                  x_1^3-\\mu_1 & x_2^3-\\mu_2 & x_3^3-\\mu_3 & . &. & x_D^3-\\mu_D \\\\\n",
    "                                  . & . & . & . &. & . \\\\\n",
    "                                  . & . & . & . &. & . \\\\\n",
    "                                  x_1^N-\\mu_1 & x_2^N-\\mu_2 & x_3^N-\\mu_3 & . &. & x_D^N-\\mu_D \\end{pmatrix}\n",
    "      \\end{bmatrix}  \n",
    "$$\n",
    "$$\n",
    "     = E\\begin{bmatrix}  \\begin{pmatrix} x_1^1 & x_2^1 & x_3^1 & . &. & x_D^1 \\\\\n",
    "                                  x_1^2 & x_2^2 & x_3^2 & . &. & x_D^2 \\\\\n",
    "                                  x_1^3 & x_2^3 & x_3^3 & . &. & x_D^3 \\\\\n",
    "                                  . & . & . & . &. & . \\\\\n",
    "                                  . & . & . & . &. & . \\\\\n",
    "                                  x_1^N & x_2^N & x_3^N & . &. & x_D^N \\end{pmatrix}^T & \n",
    "                                  \\begin{pmatrix} x_1^1 & x_2^1 & x_3^1 & . &. & x_D^1 \\\\\n",
    "                                  x_1^2 & x_2^2 & x_3^2 & . &. & x_D^2 \\\\\n",
    "                                  x_1^3 & x_2^3 & x_3^3 & . &. & x_D^3 \\\\\n",
    "                                  . & . & . & . &. & . \\\\\n",
    "                                  . & . & . & . &. & . \\\\\n",
    "                                  x_1^N & x_2^N & x_3^N & . &. & x_D^N \\end{pmatrix}\n",
    "      \\end{bmatrix} - \\ \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ . \\\\.\\\\ \\mu_D \\end{bmatrix} [\\mu_1, \\mu_2, .., \\mu_D]  \n",
    "$$\n",
    "<br>\n",
    "    * #### If two variables are related in a linear way, then the covariance will be positive or negative depending on whether the relationship has a positive or negative slope.\n",
    "    * #### But the size of the relationship is difficult to interpret because it depends on the units in which the two variables are measured.\n",
    "    * ### <span style='color:Green'>Correlation:</span>\n",
    "        * #### The _correlation_ between variable $\\mathbf{x}_i$ and $\\mathbf{x}_j$ is a statistic normalized between $-1$ and $+1$, defined as:\n",
    "          $$\n",
    "          \\Large Corr(\\mathbf{x_i},\\mathbf{x_j}) \\equiv \\rho_{ij} = \\frac{\\sigma_{ij}}{\\sigma_i \\sigma_j}          \n",
    "          $$\n",
    "    * #### If two variables are independent, then their covariance, and hence their correlation, is 0.\n",
    "    * #### However, the converse is not true: the variables may be dependent (in a nonlinear way), and their correlation may be 0.\n",
    "      <br>\n",
    "    * ### <span style='color:Green'>Sample mean:</span>\n",
    "        * #### Given a multivariate sample, The maximum likelihood estimator for the mean is the _sample mean_, $\\boldsymbol{m}$:\n",
    "          $$\n",
    "           \\Large \\boldsymbol{m}=[m_1,m_2,......m_D] = \\frac{\\sum_{t=1}^{n}\\mathbf{x}^t}{N}\n",
    "          $$\n",
    "          #### where $ \\Large m_i=\\frac{\\sum_{t=1}^{N} x_i^t}{N}=\\frac{x_i^1+x_i^2+.....+x_i^N}{N}$ is the mean of the $i^{th}$ feature.\n",
    "        * #### NOTE: Here, $N$ is the number of data samples picked from the entire population.\n",
    "          <br>\n",
    "    * ### <span style='color:Green'>Sample Covariance:</span>\n",
    "        * #### The estimator of $\\Large \\boldsymbol \\Sigma$ is $\\Large \\boldsymbol {\\mathcal{S}}$, the _sample covariance_ matrix, with entries \n",
    "          $$\n",
    "           \\Large \\mathcal{s}_i^2 = \\frac{\\sum_{t=1}^{N}(x_i^t-m_i)^2}{N-1}\n",
    "          $$\n",
    "         $$\n",
    "           \\Large \\mathcal{s}_{ij} = \\frac{\\sum_{t=1}^{N}(x_i^t-m_i)(x_j^t-m_j)}{N-1}\n",
    "          $$\n",
    "          <br>\n",
    "    * ### <span style='color:Green'>Sample Correlation:</span>\n",
    "        * #### The _sample correlation_ coefficients are:\n",
    "          $$\n",
    "           \\Large \\mathcal{r}_{ij} = \\frac{\\mathcal{s}_{ij}}{\\mathcal{s}_{i}\\mathcal{s}_{j}}\n",
    "          $$\n",
    "\n",
    "          #### and the sample correlation matrix $\\mathbf{R}$ contains $\\mathcal{r}_{ij}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3c1f0a-9597-4245-bfc7-464e6f018e98",
   "metadata": {},
   "source": [
    "* ## <span style='color:Blue'>Multivariate normal distribution:</span>\n",
    "    * #### In the multivariate case where $\\mathbf{x} \\in \\mathbb{R}^{D\\ \\mathrm{x}\\ 1}$ is the D-dimensional datapoint from normal distribution, we have\n",
    "      $$\n",
    "         \\Large p(\\mathbf{x})=\\frac{1}{(2\\pi)^{D/2}|\\boldsymbol \\Sigma|^{1/2}} e^{\\frac{-1}{2}[(\\mathbf{x}-\\boldsymbol \\mu)^T\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol \\mu)]}\n",
    "      $$\n",
    "      #### and we write $\\mathbf{x} \\sim \\mathcal{N}_D(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$, where $\\boldsymbol{\\mu}$ is the mean vector and $\\boldsymbol{\\Sigma}$ is the covariance matrix.\n",
    "    * #### In univariate normal distribution,the term $ \\frac{(x-\\mu)^2}{\\sigma^s}$ can be expanded as $(x-\\mu)(\\sigma^2)^{-1}(x-\\mu)$, and it explains the squared distance from $x$ to $\\mu$ in standard deviation units.\n",
    "      <br>\n",
    "    * #### In the multivariate case the _Mahalanobis distance_ is used:\n",
    "      $$\n",
    "        \\Large  (\\mathbf{x}-\\boldsymbol \\mu)^T\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol \\mu)\n",
    "      $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660c117f-0672-4816-8368-3c04d531f0dc",
   "metadata": {},
   "source": [
    "   * ### A special, naive case is where the components of the datapoint $\\mathbf{x}\\ \\in\\ \\mathbb{R}^{D\\ \\mathrm{x}\\ 1}$ are independent and $Cov(\\mathbf{x_i},\\mathbf{x_j} ) = 0$, for $i \\ne j $, and $Var(\\mathbf{x}_i) = \\sigma_i^2, \\forall i$.\n",
    "   * ### Then the covariance matrix is diagonal and the joint density is the product of the individual univariate densities:\n",
    "     $$\n",
    "     \\Large p(\\mathbf{x})= \\prod_{i=1}^{D} p_i(x_i)=\\frac{1}{(2\\pi)^{D/2}\\prod_{i=1}^{D}\\sigma_i} e^{[\\frac{-1}{2}\\sum_{i=1}^{D}(\\frac{x_i-\\mu_i}{\\sigma_i})^2)]}\n",
    "     $$\n",
    "     <br>\n",
    "* ### Note: Let us say the datapoint $\\mathbf{x} \\sim \\mathcal{N}_d(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$, and $\\boldsymbol{\\mathcal{w}} \\in \\mathbb{R}^D$, then\n",
    "  $$\n",
    "     \\Large \\boldsymbol{\\mathcal{w}}^T\\mathbf{x}=w_1x_1+w_2x_2+...+w_Dx_D \\sim\\ \\mathcal{N}_d(\\boldsymbol{\\mathcal{w}}^T\\boldsymbol{\\mu},\\boldsymbol{\\mathcal{w}}^T\\boldsymbol{\\Sigma}\\boldsymbol{\\mathcal{w}})\n",
    "  $$\n",
    "  ### given that\n",
    "  $$\n",
    "    \\Large  E[\\boldsymbol{\\mathcal{w}}^T\\mathbf{x}]=\\boldsymbol{\\mathcal{w}}^TE[\\mathbf{x}]=\\boldsymbol{\\mathcal{w}}^T\\boldsymbol{\\mu}\n",
    "  $$\n",
    "  $$\n",
    "  \\begin{align*} \n",
    "\\Large  Var[\\boldsymbol{\\mathcal{w}}^T\\mathbf{x}] &= \\Large E[(\\boldsymbol{\\mathcal{w}}^T\\mathbf{x}-\\boldsymbol{\\mathcal{w}}^T\\boldsymbol{\\mu})^2] \\\\ \n",
    " &= \\Large E[(\\boldsymbol{\\mathcal{w}}^T\\mathbf{x}-\\boldsymbol{\\mathcal{w}}^T\\boldsymbol{\\mu})(\\boldsymbol{\\mathcal{w}}^T\\mathbf{x}-\\boldsymbol{\\mathcal{w}}^T\\boldsymbol{\\mu})] \\\\\n",
    "  &=\\Large E[\\boldsymbol{\\mathcal{w}}^T(\\mathbf{x}-\\boldsymbol{\\mu})(\\mathbf{x}-\\boldsymbol{\\mu})^T\\boldsymbol{\\mathcal{w}}]\\\\\n",
    "  &=\\Large \\boldsymbol{\\mathcal{w}}^TE[(\\mathbf{x}-\\boldsymbol{\\mu})(\\mathbf{x}-\\boldsymbol{\\mu})^T]\\boldsymbol{\\mathcal{w}}\\\\\n",
    "  &=\\Large \\boldsymbol{\\mathcal{w}}^T\\boldsymbol{\\Sigma}\\boldsymbol{\\mathcal{w}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bcf7ea-8b16-4f3c-a9e7-a6cec72fe402",
   "metadata": {},
   "source": [
    "* ## <span style='color:Blue'>Multivariate classification:</span>\n",
    "    * ### When the datapoint $\\mathbf{x}\\in\\mathbb{R}^D$, if the class-conditional densities, $p(\\mathbf{x}|C_i)$, are taken as normal density, $\\mathcal{N}_d(\\boldsymbol{\\mu}_i,\\boldsymbol{\\Sigma}_i)$, we have\n",
    "     $$\n",
    "         \\Large p(\\mathbf{x}|C_i)=\\frac{1}{(2\\pi)^{D/2}|\\boldsymbol \\Sigma_i|^{1/2}} e^{\\frac{-1}{2}[(\\mathbf{x}-\\boldsymbol \\mu_i)^T\\boldsymbol{\\Sigma}_i^{-1}(\\mathbf{x}-\\boldsymbol \\mu_i)]}\n",
    "      $$\n",
    "    * ### $\\boldsymbol \\mu_i$, and $\\boldsymbol{\\Sigma}_i$, denotes the amount of noise in each variable and the correlations of these noise sources.\n",
    "    * ### While real data may not often be exactly multivariate normal, it is a useful approximation.\n",
    "      <br>\n",
    "    * ### Let us say we want to predict the type of a car that a customer would be interested in.\n",
    "    * ### Different cars are the classes and $\\mathbf{x}$ is the observable data of customers, for example, age and income.\n",
    "    * ### $\\boldsymbol \\mu_i$ is the vector of mean age and income of customers who buy car type $i$ and\n",
    "    * ### $\\boldsymbol \\Sigma_i$ is their covariance matrix:\n",
    "        * #### $\\sigma_{i1}^2$ and $\\sigma_{i2}^2$ are the age and income variances, and\n",
    "        * #### $\\sigma_{i12}$ is the covariance of age and income in the group of customers who buy car type $i$.\n",
    "      <br>\n",
    "    * ###  When we define the discriminant function as\n",
    "      $$\n",
    "      \\Large g_i(\\mathbf{x})=log\\ p(\\mathbf{x}|C_i)\\ +\\ log\\ P(C_i)\n",
    "      $$\n",
    "      ### and assuming $p(\\mathbf{x}|C_i)\\sim \\mathcal{N}_d(\\boldsymbol{\\mu_i},\\boldsymbol{\\Sigma_i})$, we have:\n",
    "      $$\n",
    "      \\Large g_i(\\mathbf{x})=-\\frac{d}{2}log\\ 2\\pi\\ -\\ \\frac{1}{2} log\\ |\\boldsymbol \\Sigma_i|\\ -\\ \\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu_i})^T \\boldsymbol{\\Sigma_i}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu_i}) +\\ log\\ P(C_i)\n",
    "      $$\n",
    "    <br>\n",
    "    * ### Given a training sample for $K ≥ 2$ classes, $\\chi=\\{ \\mathbf{x}^t,\\mathbf{y}^t\\}_{t=1}^N$, where\n",
    "\n",
    "\n",
    "         $$\n",
    "      \\Large    \\chi = \\begin{bmatrix}   x_1^1 & x_2^1 &. &. x_D^1 & y_1^1 & y_2^1 & . &. & y_K^1 \\\\\n",
    "                                  x_1^2 & x_2^2 &. &. x_D^2 & y_1^2 & y_2^2 & . &. & y_K^2 \\\\\n",
    "                                  x_1^3 & x_2^3 &. &. x_D^3 & y_1^3 & y_2^3 & . &. & y_K^3 \\\\\n",
    "                                  . & . & . & . &. & . \\\\\n",
    "                                  . & . & . & . &. & . \\\\\n",
    "                                  x_1^N & x_2^N &. &. x_D^N & y_1^N & y_2^N & . &. & y_K^N \\\\\n",
    "      \\end{bmatrix} \\in \\mathbb{R}^{[N\\ \\mathrm{x}\\ (D+K)]}\n",
    "$$\n",
    "      * #### From this notation, for the $t^{th}$ data instance, the input is $\\mathbf{x}^t \\in \\mathbb{R}^{[D\\mathrm{x}1]}$, which is a $D$-dimensional vector consists of $D$ features.\n",
    "      * #### Whereas, for the $t^{th}$ data instance, the output is $\\mathbf{y}^t \\in \\mathbb{R}^{[1\\ \\mathrm{x}\\ K]}$, which is a $K$ dimensional vector of the form $\\mathbf{y}^t=[y_1^t, y_2^t, . ,. , y_K^t]$\n",
    "      * #### The output vector $\\mathbf{y}^t$ follows the below definition:\n",
    "        $$\n",
    "           \\Large y_i^t=\n",
    "            \\begin{cases}\n",
    "          1 & \\mathrm{if}\\ \\mathbf{x}^t \\in C_i\n",
    "          \\\\ 0 & \\mathrm{if}\\ \\mathbf{x}^t \\in C_k, k \\ne i\n",
    "        \\end{cases}    \n",
    "        $$\n",
    "        <br>\n",
    "     \n",
    "     * #### Estimates for the unknown parameters $\\boldsymbol{\\mu_i} \\in \\mathbb{R}^{[D\\mathrm{x}1]} $ and $\\boldsymbol{\\Sigma_i} \\in \\mathbb{R}^{[D\\mathrm{x}D]}$, which are referred to as $\\boldsymbol{m}_i \\in \\mathbb{R}^{[D\\mathrm{x}1]}$ and $\\boldsymbol{S}_i \\in \\mathbb{R}^{[D\\mathrm{x}D]}$, respectively here onwards, can be estimated by the maximum likelihood estimates:\n",
    "       $$\n",
    "           \\Large \\boldsymbol{m_{i}}=\\begin{bmatrix} m_{1i} \\\\ m_{2i} \\\\.\\\\.\\\\.\\\\m_{Di} \\end{bmatrix}= \\begin{bmatrix}\\frac{\\sum_{c\\in C_i} x_1^c}{N_i}\\ \\\\ \\frac{\\sum_{c\\in C_i} x_2^c}{N_i}\\ \\\\ . \\\\.\\\\.\\\\ \\frac{\\sum_{c\\in C_i} x_D^c}{N_i} \\end{bmatrix} \\in \\mathbb{R}^{[D\\mathrm{x}1]}\n",
    "           $$\n",
    "         ##### where $\\sum_{c\\in C_i} x_d^c$ is the sum of all $d^{th}$ feature values of class $C_i$ data points, and $N_i$ is the number of class $C_i$ datapoints.\n",
    "       <br>\n",
    "    $$\n",
    "           \\Large \\boldsymbol{S_{i}}= \\frac{1}{N_i}[\\mathbf{X_i}-\\boldsymbol{m_{i}}^T]^T[\\mathbf{X_i}-\\boldsymbol{m_{i}}^T] \\in \\mathbb{R}^{[D\\mathrm{x}D]}\n",
    "           $$\n",
    "    ### where\n",
    "  $$\n",
    "      \\Large    X_i = \\begin{bmatrix}   x_1^1 & x_2^1 &. &. x_D^1 \\\\\n",
    "                                  x_1^2 & x_2^2 &. &. x_D^2  \\\\\n",
    "                                  x_1^3 & x_2^3 &. &. x_D^3 \\\\\n",
    "                                  . & . & . & .  \\\\\n",
    "                                  . & . & . & .  \\\\\n",
    "                                  x_1^{N_i} & x_2^{N_i} &. &. x_D^{N_i} \\\\\n",
    "      \\end{bmatrix} \\in \\mathbb{R}^{[N_i\\ \\mathrm{x}\\ D]}\\ \\mathrm{is\\ the\\ matrix\\ of\\ class\\ } C_i\\ \\mathrm{data\\ points}\n",
    "  $$\n",
    "\n",
    "      \n",
    "     * #### The priors $P(C_i)$ can be estimated as the ratio of number of class $C_i$ data points out of the total number of data points ($N$):\n",
    "     \n",
    "         $$\n",
    "           \\Large \\hat{P}(C_i)=\\frac{\\# class-C_i\\ data\\ points}{N}\n",
    "            $$\n",
    "     * #### These are then plugged into the discriminant function to get the estimates for the discriminants.\n",
    "     * #### Ignoring the first constant term, we have:\n",
    "       $$\n",
    "      \\Large g_i(\\mathbf{x})= -\\frac{1}{2} log\\ |\\boldsymbol{S_i}|\\ -\\ \\frac{1}{2}(\\mathbf{x}-\\boldsymbol{m_i})^T \\boldsymbol{S_i}^{-1} (\\mathbf{x}-\\boldsymbol{m_i}) +\\ log\\ \\hat{P}(C_i)\n",
    "      $$\n",
    "     * #### Expanding this, we get:\n",
    "    $$\n",
    "      \\Large g_i(\\mathbf{x})= -\\frac{1}{2} log\\ |\\boldsymbol{S_i}|\\ -\\ \\frac{1}{2}(\\mathbf{x}^T\\boldsymbol{S_i}^{-1}\\mathbf{x}-2\\mathbf{x}^T\\boldsymbol{S_i}^{-1}\\boldsymbol{m_i}+\\boldsymbol{m_i}^T \\boldsymbol{S_i}^{-1}\\boldsymbol{m_i}) +\\ log\\ \\hat{P}(C_i) \n",
    "      $$\n",
    "     * #### Thus, the data point $\\mathbf{x}$ can be assigned to the class $C_i$ that satisfies:\n",
    "     $$\n",
    "      \\Large g_i(\\mathbf{x})= \\max_{k} g_k(\\mathbf{x})\n",
    "      $$\n",
    "            \n",
    " * ### This is the likelihood-based approach to classify the multivariate data, where we use data to estimate the sample mean and variance separately, calculate the discriminant function, and then classify the data point to the class of maximum discriminant.\n",
    "\n",
    "  * ### <span style='color:Green'>Linear discriminant function:</span>\n",
    "     * #### If we assume common covariance matrix $\\boldsymbol{S_i}$ across all $K$ classes, the first term in the above equation can be ignored and the quadratic term $\\mathbf{x}^T\\boldsymbol{S_i}^{-1}\\mathbf{x}$ cancels since it is common in all discriminants. Therefore, the discriminant function becomes linear due to linear decision boundaries, and it is called as _linear discriminat function_, which can be written as:\n",
    "       $$\n",
    "      \\Large g_i(\\mathbf{x})= \\ \\mathbf{x}^T\\boldsymbol{S_i}^{-1}\\boldsymbol{m_i}-\\frac{1}{2}\\boldsymbol{m_i}^T \\boldsymbol{S_i}^{-1}\\boldsymbol{m_i} +\\ log\\ \\hat{P}(C_i) \\tag 4\n",
    "      $$\n",
    "     $$\n",
    "      \\Large g_i(\\mathbf{x})= \\ \\mathbf{w_i}^T\\mathbf{x}+w_{i0}\n",
    "      $$\n",
    "        #### where\n",
    "       $$\\Large \\mathbf{w_i}=\\boldsymbol{S_i}^{-1}\\boldsymbol{m_i}$$\n",
    "       $$\n",
    "       \\Large w_{i0}=-\\frac{1}{2}\\boldsymbol{m_i}^T \\boldsymbol{S_i}^{-1}\\boldsymbol{m_i} +\\ log\\ \\hat{P}(C_i)\n",
    "       $$\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "295ae676-0313-4c66-b1bc-b2cf2520d112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 2)\n",
      "(20, 2)\n",
      "(20, 2)\n",
      "(60, 2)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy import random\n",
    "import numpy as np\n",
    "x1_C1 = random.normal(loc=1, scale=2, size=(20, 1))\n",
    "x2_C1 = random.normal(loc=1, scale=2, size=(20, 1))\n",
    "x1_C2 = random.normal(loc=10, scale=2, size=(20, 1))\n",
    "x2_C2 = random.normal(loc=30, scale=2, size=(20, 1))\n",
    "x1_C3 = random.normal(loc=20, scale=2, size=(20, 1))\n",
    "x2_C3 = random.normal(loc=5, scale=2, size=(20, 1))\n",
    "\n",
    "\n",
    "x1=np.concatenate((x1_C1,x1_C2,x1_C3), axis = 0)\n",
    "x2=np.concatenate((x2_C1,x2_C2,x2_C3), axis = 0)\n",
    "\n",
    "plt.scatter(x1_C1, x2_C1, color='blue', label='Group 1')\n",
    "plt.scatter(x1_C2, x2_C2, color='red', label='Group 2')\n",
    "plt.scatter(x1_C3, x2_C3, color='green', label='Group 3')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "X=np.concatenate((x1,x2), axis = 1)\n",
    "\n",
    "X_C1=np.concatenate((x1_C1,x2_C1), axis = 1)\n",
    "X_C2=np.concatenate((x1_C2,x2_C2), axis = 1)\n",
    "X_C3=np.concatenate((x1_C3,x2_C3), axis = 1)\n",
    "print(X_C1.shape)\n",
    "print(X_C2.shape)\n",
    "print(X_C3.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a1801a-552a-4b46-bf14-0ab83334b8f5",
   "metadata": {},
   "source": [
    "#### Computing\n",
    " $$\n",
    "           \\Large \\boldsymbol{m_{i}}= \\begin{bmatrix}\\frac{\\sum_{c\\in C_i} x_1^c}{N_i}\\ \\\\ \\frac{\\sum_{c\\in C_i} x_2^c}{N_i}\\ \\\\ . \\\\.\\\\.\\\\ \\frac{\\sum_{c\\in C_i} x_D^c}{N_i} \\end{bmatrix} \\in \\mathbb{R}^{[D\\mathrm{x}1]}\n",
    "           $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "495d7f83-a1a8-4d89-a175-05e82224e77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9365583449097639\n",
      "0.9246221038734994\n",
      "10.303075165617235\n",
      "29.677639804119792\n",
      "20.0945565537992\n",
      "4.975257397299117\n",
      "(2,)\n",
      "(2,)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "m_1_C1=np.divide(np.sum(x1_C1),len(x1_C1))\n",
    "print(m_1_C1)\n",
    "\n",
    "m_2_C1=np.divide(np.sum(x2_C1),len(x2_C1))\n",
    "print(m_2_C1)\n",
    "\n",
    "m_1_C2=np.divide(np.sum(x1_C2),len(x1_C2))\n",
    "print(m_1_C2)\n",
    "\n",
    "m_2_C2=np.divide(np.sum(x2_C2),len(x2_C2))\n",
    "print(m_2_C2)\n",
    "\n",
    "m_1_C3=np.divide(np.sum(x1_C3),len(x1_C3))\n",
    "print(m_1_C3)\n",
    "\n",
    "m_2_C3=np.divide(np.sum(x2_C3),len(x2_C3))\n",
    "print(m_2_C3)\n",
    "\n",
    "m_C1=np.array([m_1_C1,m_2_C1])\n",
    "m_C2=np.array([m_1_C2,m_2_C2])\n",
    "m_C3=np.array([m_1_C3,m_2_C3])\n",
    "print(m_C1.shape)\n",
    "print(m_C2.shape)\n",
    "print(m_C3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db436bc5-054b-4fa9-91e1-33d559a2ad26",
   "metadata": {},
   "source": [
    "#### Computing\n",
    "$$\n",
    "           \\Large \\boldsymbol{S_{i}}= \\frac{1}{N_i}[\\mathbf{X_i}-\\boldsymbol{m_{i}}^T]^T[\\mathbf{X_i}-\\boldsymbol{m_{i}}^T] \\in \\mathbb{R}^{[D\\mathrm{x}D]}\n",
    "           $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe66a630-02c2-4d04-9502-c7b2dd1aa172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.07447637 1.58309249]\n",
      " [1.58309249 4.22620071]]\n",
      "[[ 2.36626944 -0.24563313]\n",
      " [-0.24563313  5.03290363]]\n",
      "[[ 3.1683163  -0.54026058]\n",
      " [-0.54026058  4.16650639]]\n"
     ]
    }
   ],
   "source": [
    "S_C1=np.divide(np.dot(np.transpose(X_C1-m_C1),X_C1-m_C1),len(X_C1))\n",
    "S_C2=np.divide(np.dot(np.transpose(X_C2-m_C2),X_C2-m_C2),len(X_C2))\n",
    "S_C3=np.divide(np.dot(np.transpose(X_C3-m_C3),X_C3-m_C3),len(X_C3))\n",
    "print(S_C1)\n",
    "print(S_C2)\n",
    "print(S_C3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4e40b2-2222-462c-a8d4-cee0802ba672",
   "metadata": {},
   "source": [
    "#### Computing priors\n",
    " $$\n",
    "           \\Large \\hat{P}(C_i)=\\frac{\\# class-C_i\\ data\\ points}{N}\n",
    "            $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2ec8a8e-f63d-4bde-a187-1c5e0d65d2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "0.3333333333333333\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "P_hat_C1=np.divide(len(X_C1),len(X))\n",
    "print(np.mean(P_hat_C1))\n",
    "\n",
    "P_hat_C2=np.divide(len(X_C2),len(X))\n",
    "print(np.mean(P_hat_C2))\n",
    "\n",
    "P_hat_C3=np.divide(len(X_C3),len(X))\n",
    "print(np.mean(P_hat_C3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad23705-1372-4df5-8a66-3e4bf657a974",
   "metadata": {},
   "source": [
    "\n",
    "### Computing linear discriminant function: \n",
    "$$\\Large \\mathbf{w_i}=\\boldsymbol{S_i}^{-1}\\boldsymbol{m_i}$$\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "915e9fa8-8329-4a12-bd8d-e50bbe3c232e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_1=np.dot(np.linalg.inv(S_C1),m_C1)\n",
    "w_2=np.dot(np.linalg.inv(S_C2),m_C2)\n",
    "w_3=np.dot(np.linalg.inv(S_C3),m_C3)\n",
    "w_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a836bc-9d74-4230-af5a-0132cd53ac89",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\Large   w_{i0}=-\\frac{1}{2}\\boldsymbol{m_i}^T \\boldsymbol{S_i}^{-1}\\boldsymbol{m_i} +\\ log\\ \\hat{P}(C_i)\n",
    "       $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e662a623-d40f-40ba-9767-76a9ed72df8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2699467686379327"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_10=-(1/2)*np.dot(np.dot(np.transpose(m_C1),np.linalg.inv(S_C1)),m_C1)+np.log(P_hat_C1)\n",
    "w_20=-(1/2)*np.dot(np.dot(np.transpose(m_C2),np.linalg.inv(S_C2)),m_C2)+np.log(P_hat_C2)\n",
    "w_30=-(1/2)*np.dot(np.dot(np.transpose(m_C3),np.linalg.inv(S_C3)),m_C3)+np.log(P_hat_C3)\n",
    "w_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede6aede-22a5-4554-b970-afbc32d1b6c1",
   "metadata": {},
   "source": [
    "$$\n",
    "      \\Large g_i(\\mathbf{x})= \\ \\mathbf{w_i}^T\\mathbf{x}+w_{i0}\n",
    "      $$\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8202d1ba-078c-4cb4-a708-740eee8a640c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.32105332e-01 -9.35350570e+01 -5.68563565e+01]\n",
      " [-4.78996614e-01 -9.51874700e+01 -5.40764222e+01]\n",
      " [-3.52104694e-01 -8.03768423e+01 -5.61475483e+01]\n",
      " [-1.63454264e+00 -1.30102597e+02 -8.16450440e+01]\n",
      " [-8.41696043e-01 -1.06578766e+02 -6.25288123e+01]\n",
      " [-3.66979781e-01 -9.01089412e+01 -5.21907629e+01]\n",
      " [-7.11815348e-01 -1.03730858e+02 -5.89301557e+01]\n",
      " [-4.36784769e-01 -9.86596497e+01 -5.08641366e+01]\n",
      " [-2.31143777e+00 -1.45736364e+02 -1.00032446e+02]\n",
      " [-1.28506373e+00 -1.14439992e+02 -7.56775779e+01]\n",
      " [-1.90278705e+00 -1.40638141e+02 -8.69158054e+01]\n",
      " [-1.47098207e+00 -1.21999042e+02 -7.92113620e+01]\n",
      " [-7.64271625e-01 -1.02592198e+02 -6.14467360e+01]\n",
      " [-1.44287523e+00 -1.18701905e+02 -7.96778082e+01]\n",
      " [-7.72770507e-01 -1.08849951e+02 -5.88621312e+01]\n",
      " [-9.99481703e-01 -1.06374301e+02 -6.86026395e+01]\n",
      " [-1.25408555e+00 -1.21157276e+02 -7.13836516e+01]\n",
      " [ 4.70703629e-03 -7.21204618e+01 -4.64621281e+01]\n",
      " [-7.15357401e-01 -1.09209969e+02 -5.65193951e+01]\n",
      " [-2.76125355e-01 -9.14153567e+01 -4.81412698e+01]\n",
      " [ 5.36909079e+00  1.25754360e+02  6.48961750e+01]\n",
      " [ 5.02401370e+00  1.12504324e+02  5.79748959e+01]\n",
      " [ 4.81689621e+00  1.11787415e+02  5.04597407e+01]\n",
      " [ 5.25519963e+00  1.18004413e+02  6.41803327e+01]\n",
      " [ 4.87656405e+00  1.17680785e+02  4.99832947e+01]\n",
      " [ 4.95754110e+00  1.29899047e+02  4.73764714e+01]\n",
      " [ 4.53803278e+00  9.46072169e+01  4.78729579e+01]\n",
      " [ 4.91056238e+00  1.10786943e+02  5.44736703e+01]\n",
      " [ 5.48244622e+00  1.34698880e+02  6.50368530e+01]\n",
      " [ 5.04989895e+00  1.15231469e+02  5.76890225e+01]\n",
      " [ 4.85259421e+00  1.05042565e+02  5.49453159e+01]\n",
      " [ 4.87742058e+00  1.03695331e+02  5.65118161e+01]\n",
      " [ 4.14351348e+00  8.80058391e+01  3.59899847e+01]\n",
      " [ 6.19956376e+00  1.55931728e+02  8.23476716e+01]\n",
      " [ 5.19012029e+00  1.28493010e+02  5.68425095e+01]\n",
      " [ 5.38006771e+00  1.18666263e+02  6.86044459e+01]\n",
      " [ 4.51777422e+00  1.04168188e+02  4.26643658e+01]\n",
      " [ 4.68995015e+00  9.85453952e+01  5.18002169e+01]\n",
      " [ 4.70351077e+00  1.10180560e+02  4.69096732e+01]\n",
      " [ 5.75423255e+00  1.30934262e+02  7.70840690e+01]\n",
      " [ 3.38323721e+00 -1.54337072e+01  5.52277491e+01]\n",
      " [ 4.20779789e+00  2.23152864e+01  6.89382923e+01]\n",
      " [ 4.16402987e+00  1.95923521e+01  6.85445914e+01]\n",
      " [ 3.56325989e+00  1.21517224e+00  5.43160233e+01]\n",
      " [ 4.77634180e+00  2.95174884e+01  8.71363763e+01]\n",
      " [ 4.39040923e+00  2.33684822e+01  7.53686531e+01]\n",
      " [ 4.30661975e+00  1.56436111e+01  7.57817844e+01]\n",
      " [ 4.65740304e+00  2.14494692e+01  8.63770098e+01]\n",
      " [ 3.41882279e+00 -1.33229959e+01  5.55957690e+01]\n",
      " [ 3.88036151e+00 -1.47908587e+00  6.75831700e+01]\n",
      " [ 4.05229505e+00 -4.35016086e+00  7.54316985e+01]\n",
      " [ 4.19674961e+00  2.09058207e+01  6.91743269e+01]\n",
      " [ 4.74664863e+00  3.11899681e+01  8.52343894e+01]\n",
      " [ 4.22842982e+00  7.03214506e+00  7.68189073e+01]\n",
      " [ 3.49584219e+00  3.12207128e+00  5.08756836e+01]\n",
      " [ 4.06621824e+00  1.88090903e+01  6.52021022e+01]\n",
      " [ 4.45756423e+00  2.03418357e+01  7.93191466e+01]\n",
      " [ 4.75426619e+00  3.08534474e+01  8.56793455e+01]\n",
      " [ 3.55104544e+00  2.21026629e-03  5.44165956e+01]\n",
      " [ 4.79622522e+00  2.77188003e+01  8.87252718e+01]]\n"
     ]
    }
   ],
   "source": [
    "g_1=np.dot(X,w_1)+w_10\n",
    "g_2=np.dot(X,w_2)+w_20\n",
    "g_3=np.dot(X,w_3)+w_30\n",
    "\n",
    "g_1_new = g_1[:, np.newaxis]\n",
    "g = np.concatenate((g_1_new, g_2[:, np.newaxis],g_3[:, np.newaxis]), axis=1)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e062d3-0a0a-4cbb-9ea0-e565480fff59",
   "metadata": {},
   "source": [
    "#### Classify the data $X$ as\n",
    " $$\n",
    "      \\Large g_i(\\mathbf{x})= \\max_{k} g_k(\\mathbf{x})\n",
    "      $$\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "326df361-8bdb-4bfd-a01f-79902d03ed5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The datapoint x classified to class: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"The datapoint x classified to class:\",np.argmax(g,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f9c97e-2209-4ea3-a187-1da28b2cd5e3",
   "metadata": {},
   "source": [
    "     <br>\n",
    " * ### <span style='color:Green'>Naive Bayes' classifier:</span>\n",
    "    * #### Further simplification of the discriminant function $g_i(\\mathbf{x})$ of Equation 4 may be possible by assuming all off-diagonals of the covariance matrix to be 0, thus assuming features to be independent.\n",
    "      $$\n",
    "      \\Large g_i(\\mathbf{x})= \\ \\mathbf{x}^T\\boldsymbol{S_i}^{-1}\\boldsymbol{m_i}-\\frac{1}{2}\\boldsymbol{m_i}^T \\boldsymbol{S_i}^{-1}\\boldsymbol{m_i} +\\ log\\ \\hat{P}(C_i)\n",
    "      $$\n",
    "   * #### This is the naive Bayes’ classifier where $p(x_j|C_i)$ are univariate Gaussian.\n",
    "   * #### It is possible to pool the data and estimate a common covariance matrix for all classes:\n",
    "     $$\n",
    "     \\Large \\boldsymbol{S}=\\sum_{i} \\hat{P}(C_i)\\boldsymbol{S_i}\n",
    "     $$\n",
    "   * #### This can changes the discriminat function into:\n",
    "     $$\n",
    "      \\Large g_i(\\mathbf{x})= \\ \\mathbf{x}^T\\boldsymbol{S}^{-1}\\boldsymbol{m_i}-\\frac{1}{2}\\boldsymbol{m_i}^T \\boldsymbol{S}^{-1}\\boldsymbol{m_i} +\\ log\\ \\hat{P}(C_i)\n",
    "      $$\n",
    "     <br>\n",
    "   * #### $\\boldsymbol{S}$ and its inverse are diagonal, and for the data point $\\mathbf{x}=[x_1,x_2,....x_D]$ we get:\n",
    "     $$\n",
    "      \\Large g_i(\\mathbf{x})=-\\frac{1}{2} \\left[ \\left(\\frac{x_1-m_{1i}}{s_{1}^2}\\right)^2 + \\left(\\frac{x_2-m_{2i}}{s_{2}^2}\\right)^2 + ....\\left(\\frac{x_D-m_{Di}}{s_{D}^2}\\right)^2 \\right]+\\ log\\ \\hat{P}(C_i)\n",
    "     $$\n",
    "     $$\n",
    "      \\Large g_i(\\mathbf{x})=-\\frac{1}{2}  \\sum_{d=1}^D \\left(\\frac{x_d-m_{di}}{s_{d}^2} \\right)^2 +\\ log\\ \\hat{P}(C_i)\n",
    "     $$\n",
    "   #### where $m_{di}$ is the mean of all $d^{th}$ feature points belongs to class - $C_i$, and $s_{d}^2$ is the variance of $d^{th}$ feature points.\n",
    "   * #### The term $\\Large \\left(\\frac{x_d-m_{di}}{s_{d}^2}\\right)$ has the effect of normalization and measures the distance in terms of standard deviation units.\n",
    "   * #### Geometrically speaking, classes are hyperellipsoidal and, because the covariances are zero, are axis-aligned.\n",
    "     <br>\n",
    "* ### <span style='color:Green'>Nearest mean classifier:</span>\n",
    "    * #### Simplifying the $g_i(\\mathbf{x})$ even further, if we assume all variances to be equal, the Mahalanobis distance reduces to _Euclidean distance_.\n",
    "    * #### Geometrically, the distribution is shaped spherically, centered around the mean vector $\\boldsymbol{m}_i$\n",
    "      $$\n",
    "      \\Large g_i(\\mathbf{x})=-||\\mathbf{x}-\\boldsymbol{m}_i||^2 +\\ log\\ \\hat{P}(C_i)\n",
    "     $$\n",
    "    * #### If the priors are equal, we have\n",
    "      $$\n",
    "      \\Large g_i(\\mathbf{x})=-||\\mathbf{x}-\\boldsymbol{m}_i||^2 \n",
    "     $$\n",
    "    * #### This is named the nearest mean classifier because it assigns the input to the class of the nearest mean.\n",
    "      $$\n",
    "      \\Large g_i(\\mathbf{x})=-||\\mathbf{x}-\\boldsymbol{m}_i||^2 = -(\\mathbf{x}-\\boldsymbol{m}_i)^T(\\mathbf{x}-\\boldsymbol{m}_i)\n",
    "     $$\n",
    "     $$\n",
    "      \\Large g_i(\\mathbf{x})= -(\\mathbf{x}^T\\mathbf{x}-2\\boldsymbol{m}_i^T\\mathbf{x}+\\boldsymbol{m}_i^T\\boldsymbol{m}_i)\n",
    "     $$\n",
    "    * #### The first term, $\\mathbf{x}^T\\mathbf{x}$, is shared in all $g_i(\\mathbf{x})$ and can be dropped, and we can write the discriminant function as\n",
    "      $$\n",
    "      \\Large g_i(\\mathbf{x})= \\mathbf{w}_i^T\\mathbf{x}+w_{i0}\n",
    "     $$\n",
    "      #### where $\\mathbf{w}_i=\\boldsymbol{m}_i$ and $w_{i0}=-\\frac{1}{2}||\\boldsymbol{m}_i||^2$\n",
    "    * #### If all $\\boldsymbol{m_i}$ have similar norms, then this term can also be ignored and we can use\n",
    "      $$\n",
    "       \\Large g_i(\\mathbf{x})= \\boldsymbol{m}_i^T\\mathbf{x}\n",
    "      $$\n",
    "    * #### When the norms of $\\boldsymbol{m}_i$ are comparable, dot product can also be used as the similarity measure instead of the (negative) Euclidean distance.\n",
    "* ## We can actually think of finding the best discriminant function as the task of finding the best distance function.\n",
    "* ## This can be seen as another approach to classification: Instead of learning the discriminant functions, $g_i(\\mathbf{x})$, we want to learn the suitable distance function $D(\\mathbf{x}1, \\mathbf{x}2)$, such that for any $\\mathbf{x}_1$, $\\mathbf{x}_2$, $\\mathbf{x}_3$, where $\\mathbf{x}_1$ and $\\mathbf{x}_2$ belong to the same class, and $\\mathbf{x}_1$ and $\\mathbf{x}_3$ belong to two different classes, we would like to have\n",
    "    $$\n",
    "      \\Large  D(\\mathbf{x}_1, \\mathbf{x}_2) < D(\\mathbf{x}_1, \\mathbf{x}_3)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c2031-3618-4869-8320-a50e43ad0c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
