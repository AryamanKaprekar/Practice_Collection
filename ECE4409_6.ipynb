{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd5e012f-b35f-428e-9ca9-03378159b26d",
   "metadata": {},
   "source": [
    "# <span style='color:Red'>Linear discrimination:</span>[Ref: Chapter 10 of [[1]](https://erp.metbhujbalknowledgecity.ac.in/StudyMaterial/01VM092015008350131.pdf)]\n",
    "   * ### In linear discriminant, the discriminant functions are linear in $\\mathbf{x}$:\n",
    "      $$\n",
    "       \\Large  g_i(\\mathbf{x}|\\mathbf{w_i},w_{i0})=\\mathbf{w_i}^T\\mathbf{x}+w_{i0}=\\sum_{j=1}^d w_{ij}x_j+w_{i0}\n",
    "     $$\n",
    "   * ### The linear model is easy to understand:\n",
    "       * #### The final output is a weighted sum of the input attributes $x_j$.\n",
    "       * #### The magnitude of the weight $w_j$ shows the importance of $x_j$ and its sign indicates if the effect is positive or negative.\n",
    "       * #### Most functions are additive in that the output is the sum of the effects of several attributes where the weights may be positive (enforcing) or negative (inhibiting).\n",
    "       * #### For example, when a customer applies for credit, financial institutions calculate the applicant’s credit score that is generally written as a sum of the effects of various attributes; for example, yearly income has a positive effect (higher incomes increase the score).\n",
    "       * #### In many applications, the linear discriminant is also quite accurate.\n",
    "       * #### We know, for example, that when classes are Gaussian with a shared covariance matrix, the optimal discriminant is linear.\n",
    "       * #### The linear discriminant, however, can be used even when this assumption does not hold, and the model parameters can be calculated without making any assumptions on the class densities.\n",
    "       * #### We should always use the linear discriminant before trying a more complicated model to make sure that the additional complexity is justified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573461d7-53b7-4357-afae-ef171e30d475",
   "metadata": {},
   "source": [
    " * ## <span style='color:Blue'>Geometry of linear discriminant:</span>\n",
    "     * ### <span style='color:Green'>Two classes:</span>\n",
    "         * #### In two classes discrimination, one discriminant function is sufficient:\n",
    "           $$\n",
    "            \\Large g(\\mathbf{x})= g_1(\\mathbf{x})- g_2(\\mathbf{x})\n",
    "           $$\n",
    "           $$\n",
    "            \\Large g(\\mathbf{x})= (\\mathbf{w_1}^T\\mathbf{x}+w_{10})- (\\mathbf{w_2}^T\\mathbf{x}+w_{20})\n",
    "           $$\n",
    "           $$\n",
    "            \\Large g(\\mathbf{x})= (\\mathbf{w_1}-\\mathbf{w_2})^T\\mathbf{x}+(w_{10}-w_{20}))\n",
    "           $$\n",
    "           $$\n",
    "            \\Large g(\\mathbf{x})= \\mathbf{w}^T\\mathbf{x}+w_0\n",
    "           $$\n",
    "        ### and we \n",
    "\n",
    "   $$\n",
    "       \\Large\n",
    "       choose \\begin{cases}\n",
    "       C=1 & \\mathrm{if}\\ g(\\mathbf{x}) > 0\\\\\n",
    "      C=0 & \\mathrm{otherwise}\n",
    "        \\end{cases}    \n",
    "       $$\n",
    "   * ### This defines a hyperplane where $\\mathbf{w}$ is the _weight vector_ and $w_0$ is the _threshold_.\n",
    "   * ### This _threshold_ name comes from the fact that the decision rule can be rewritten as follows:\n",
    "      $$\n",
    "       \\Large\n",
    "       choose \\begin{cases}\n",
    "       C_1 & \\mathrm{if}\\ \\mathbf{w}^T\\mathbf{x} > −w_0\\\\\n",
    "      C_2 & \\mathrm{otherwise}\n",
    "        \\end{cases}    \n",
    "       $$\n",
    "     &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;    <img src=\"images/Linear_Discriminant.png\" width=\"600\" height=\"300\">\n",
    "   * ### The hyperplane divides the input space into two half-spaces:\n",
    "       * #### The decision region $\\mathcal{R}_1$ for $C_1$ and $\\mathcal{R}_2$ for $C_2$.\n",
    "       * #### Any $\\mathbf{x}$ in $\\mathcal{R}_1$ is on the positive side of the hyperplane and any $\\mathbf{x}$ in $\\mathcal{R}_2$ is on its negative side.\n",
    "       * #### When $\\mathbf{x}$ is $0$, $g(\\mathbf{x})$ = $w_0$ and we see that if $w_0 > 0$, the origin is on the positive side of the hyperplane, and if $w_0 < 0$, the origin is on the negative side, and if $w_0 = 0$, the hyperplane passes through the origin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d0e68c-91e7-4d1a-84ea-28e562e8d8c6",
   "metadata": {},
   "source": [
    "# <span style='color:Red'>Logistic regression:</span>\n",
    "   * ### Logistic regression is a supervised machine learning algorithm used for classification tasks where the goal is to predict the probability that an instance belongs to a given class or not.\n",
    "   * ### Logistic regression is used for binary classification where we use a sigmoid function that takes input as independent variables and produces a probability value between 0 and 1.\n",
    "   * ### For example, we have two classes Class 0 and Class 1. If the value of the logistic function for an input is greater than 0.5 (threshold value), then it belongs to Class 1; otherwise, it belongs to Class 0.\n",
    "   * ### It’s referred to as regression because it is the extension of linear regression but is mainly used for classification problems.\n",
    " * ## <span style='color:Blue'>Logit transformation (or) log odds:</span>\n",
    "     * ### In logistic regression, the relationship between the independent variables and the dependent variable is expressed through log-odds.\n",
    "     * ### Odds are nothing but the ratio of the probability of success and the probability of failure.\n",
    "     * ### For example, In the binary classification problem of class 0 ($C_0$) and class 1($C_1$) input data, the logit transformation of the posterior probability $P(C_1|\\mathbf{x})$ is:\n",
    "       $$\n",
    "       \\Large logit(P(C_1|\\mathbf{x}))=\\log \\frac{P(C_1|\\mathbf{x})}{1-P(C_1|\\mathbf{x})}= \\log \\frac{P(C_1|\\mathbf{x})}{P(C_0|\\mathbf{x})}\n",
    "       $$\n",
    "     * ### It has been seen earlier that the posterior $P(C_1|\\mathbf{x})$ is proportional to the product of likelihood $P(\\mathbf{x}|C_1)$ and prior $P(C_1)$.\n",
    "     * ### Therefore, the $ logit(P(C_1|\\mathbf{x}))$ can be restructured as:\n",
    "       $$\n",
    "        \\begin{align*}\n",
    "       \\Large logit(P(C_1|\\mathbf{x}))&=\\log \\frac{P(C_1|\\mathbf{x})}{P(C_0|\\mathbf{x})}\\\\\n",
    "       &=\\log \\frac{P(\\mathbf{x}|C_1)P(C_1)}{P(\\mathbf{x}|C_0)P(C_0)}\\\\\n",
    "       &=\\{log\\ p(\\mathbf{x}|C_1)\\ +\\ log\\ P(C_1)\\}-\\{log\\ p(\\mathbf{x}|C_0)\\ +\\ log\\ P(C_0)\\}\n",
    "       \\end{align*}\n",
    "       $$\n",
    "      * ### It has been observed in FILE _\"ECE4409_5.ipynb\"_ that the discriminant function $g_i(\\mathbf{x})$ in the case of common covariance matrix across all classes looks like:\n",
    " \n",
    "     $$\n",
    "      \\Large g_i(\\mathbf{x})=log\\ p(\\mathbf{x}|C_i)\\ +\\ log\\ P(C_i)= \\ \\mathbf{x}^T\\boldsymbol{S_i}^{-1}\\boldsymbol{m_i}-\\frac{1}{2}\\boldsymbol{m_i}^T \\boldsymbol{S_i}^{-1}\\boldsymbol{m_i} +\\ log\\ \\hat{P}(C_i)\n",
    "      $$\n",
    "      * ### This can be written as:\n",
    "     $$\n",
    "      \\Large g_i(\\mathbf{x})= \\ \\mathbf{w_i}^T\\mathbf{x}+w_{i0}\n",
    "      $$\n",
    "     #### where\n",
    "   $$\\Large \\mathbf{w_i}=\\boldsymbol{S_i}^{-1}\\boldsymbol{m_i}$$\n",
    "       $$\n",
    "       \\Large w_{i0}=-\\frac{1}{2}\\boldsymbol{m_i}^T \\boldsymbol{S_i}^{-1}\\boldsymbol{m_i} +\\ log\\ \\hat{P}(C_i)\n",
    "       $$\n",
    "    * ### Now, the _logit_ function can be rewritten as:\n",
    "      $$\n",
    "       \\Large \n",
    "       \\begin{align*}\n",
    "           logit(P(C_1|\\mathbf{x}))&=g_1(\\mathbf{x})-g_0(\\mathbf{x})\\\\\n",
    "           &=(\\mathbf{w_1}^T\\mathbf{x}+w_{10})-(\\mathbf{w_0}^T\\mathbf{x}+w_{00})\\\\\n",
    "           &=(\\mathbf{w_1}-\\mathbf{w_0})^T\\mathbf{x}+(w_{10}-w_{00})\\\\\n",
    "           &=\\mathbf{w}^T\\mathbf{x}+w_{0}\n",
    "           \\end{align*} \\tag 1\n",
    "       $$\n",
    "     #### where\n",
    "   $$\\mathbf{w}=\\boldsymbol{S_i}^{-1}(\\boldsymbol{m_1}-\\boldsymbol{m_2})$$\n",
    "   $$ w_{0} = -\\frac{1}{2}(\\boldsymbol{m_1}+\\boldsymbol{m_2})^T \\boldsymbol{S_i}^{-1} (\\boldsymbol{m_1}-\\boldsymbol{m_2})+\\log \\frac{P(C_1)}{P(C_0)}$$\n",
    "   <br>\n",
    "  * ### From the set of equations (1) above, the _logit_ transformation of the posterior probability $P(C_1|\\mathbf{x})$ can be viewed as the output of LINEAR REGRESSION.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc75e3c7-6106-4174-85fc-97a67eda4880",
   "metadata": {},
   "source": [
    " * ## <span style='color:Blue'>Sigmoid function</span>\n",
    "* ### The _logit_ transformation can further be expanded as:\n",
    "    $$\n",
    "       \\Large \n",
    "      logit(P(C_1|\\mathbf{x}))= \\log \\frac{P(C_1|\\mathbf{x})}{1-P(C_1|\\mathbf{x})} = \\mathbf{w}^T\\mathbf{x}+w_{0}\n",
    "     $$\n",
    "    $$\n",
    "      \\Large     \\frac{P(C_1|\\mathbf{x})}{1-P(C_1|\\mathbf{x})} = e^{(\\mathbf{w}^T\\mathbf{x}+w_{0})}\n",
    "$$\n",
    "  $$\n",
    "     \\Large    P(C_1|\\mathbf{x})=e^{(\\mathbf{w}^T\\mathbf{x}+w_{0})}(1-P(C_1|\\mathbf{x}))\n",
    "$$\n",
    "  $$\n",
    "       \\Large     P(C_1|\\mathbf{x})(1+e^{(\\mathbf{w}^T\\mathbf{x}+w_{0})})=e^{(\\mathbf{w}^T\\mathbf{x}+w_{0})}\n",
    "    $$  \n",
    " $$\n",
    "       \\Large     P(C_1|\\mathbf{x})=\\frac{e^{(\\mathbf{w}^T\\mathbf{x}+w_{0})}}{1+e^{(\\mathbf{w}^T\\mathbf{x}+w_{0})}}\n",
    "    $$\n",
    "\n",
    "     $$\n",
    "       \\Large     P(C_1|\\mathbf{x})=\\frac{1}{1+e^{-(\\mathbf{w}^T\\mathbf{x}+w_{0})}} \\ \\  ==> Sigmoid\\ function\n",
    "    $$\n",
    "  * ### From the LINEAR REGRESSION, it has been learned that the predicted output $'z'$ satisfies $z=\\mathbf{w}^T\\mathbf{x}+w_{0}$\n",
    "  * ### Therefore, the $P(C_1|\\mathbf{x})$ in terms of the output of linear regression is: \n",
    "    $$\n",
    "       \\Large  P(C_1|\\mathbf{x})=\\frac{1}{1+e^{-z}} = \\sigma(z)=\\frac{1}{1+e^{-z}}\n",
    "    $$\n",
    "    #### where $\\sigma(z)$ is the __sigmoid function__ of input $z$.\n",
    "  * ### The sigmoid function transforms any real value input into the probabilities between 0 and 1.\n",
    "  &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;  <img src=\"images/Sigmoid.png\" width=\"400\" height=\"200\">\n",
    "  * ### As shown above, the figure sigmoid function converts the continuous variable data into the probability i.e. between 0 and 1. \n",
    "  * ### $\\sigma(z)$ tend towards $1$ as $z\\rightarrow \\infty$\n",
    "  * ### $\\sigma(z)$ tend towards $0$ as $z\\rightarrow -\\infty$\n",
    "  * ### $\\sigma(z)$  is always bounded between $0$ and $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0df2cb5-0f48-4fed-a629-b6403a8fe23b",
   "metadata": {},
   "source": [
    "## <span style='color:Blue'>Loss function of Logistic Regression:</span>\n",
    " &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;  <img src=\"images/Logistic_Regression.png\" width=\"600\" height=\"300\">\n",
    " * ### In Binary classification:\n",
    "   *  #### For the data $\\mathbf{X}=\\{\\mathbf{x}^t,y^t\\}$, the output\n",
    "      $$\n",
    "   y^t=1,\\ \\mathrm{if}\\ \\mathbf{x}^t \\in C_1 \n",
    "   $$\n",
    "   $$\n",
    "   y^t=0,\\ \\mathrm{if}\\ \\mathbf{x}^t \\in C_0 \n",
    "   $$\n",
    "   * #### We assume, output $y^t$, given $\\mathbf{x}^t$, is Bernoulli with probability $y^t \\equiv P(C_1|\\mathbf{x}^t)=p^t$:\n",
    "     $$\n",
    "     y^t|\\mathbf{x}^t \\sim Bernoulli (p^t)\n",
    "     $$\n",
    "   * #### Therefore, the sample likelihood is:\n",
    "     $$\n",
    "     \\mathcal{l}(\\mathbf{w}|\\mathbf{X})=\\prod_{t}(p^t)^{y^t}(1-p^t)^{1-y^t}\n",
    "     $$\n",
    "   * #### The error or loss function of logistic regression is:\n",
    "     $$\n",
    "     E(\\mathbf{w}|\\mathbf{X})=-\\sum_t y^t \\log p^t + (1-y^t) \\log (1-p^t)\n",
    "     $$\n",
    "   * #### The goal is to compute the weights $\\mathbf{w}=[w_0, w_1, ....w_D]^T$ that minimizes the error function $E$:\n",
    "     $$\n",
    "      \\Large \\mathbf{w}^*=\\arg \\min_{\\mathbf{w}} E(\\mathbf{w}|\\mathbf{X})\n",
    "     $$\n",
    "   * #### Since the output of logistic regression is $\\sigma(\\mathbf{w}^T\\mathbf{x})$, it holds a non linear relationship with the input $\\mathbf{x}$. Therefore, an analytical solution that minimizes the error function $E$ can not be computed directly.\n",
    "   * #### We use gradient descent to minimize the error function $E$, equivalent to maximizing the likelihood or log likelihood.\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346ec537-d2eb-4f59-b3b0-ee949ef3953a",
   "metadata": {},
   "source": [
    "## <span style='color:Blue'>Gradient descent for Logistic Regression:</span>\n",
    " * ### The gradient descent equation for the logistic regression is:\n",
    "   $$\n",
    "     \\Large \\mathbf{w}^{i+1}=\\mathbf{w}^{i}-\\eta \\frac{\\partial E}{\\partial \\mathbf{w}^{i}}\n",
    "    $$\n",
    "   ### where '$\\eta$' is the learning rate; and '$i$' is the iteration.\n",
    "  * ### In terms of the individual weights:\n",
    "  $$\n",
    "  \\Large\n",
    "  \\begin{bmatrix}\n",
    "  w_0\\\\\n",
    "  w_1\\\\\n",
    "  .\\\\\n",
    "  .\\\\\n",
    "  w_D\n",
    "\\end{bmatrix}^{i+1}=\n",
    "\\begin{bmatrix}\n",
    "  w_0\\\\\n",
    "  w_1\\\\\n",
    "  .\\\\\n",
    "  .\\\\\n",
    "  w_D\n",
    "\\end{bmatrix}^{i}-\\eta\n",
    "\\begin{bmatrix}\n",
    "  \\frac{\\partial E}{\\partial w_0^{i}}\\\\\n",
    "  \\frac{\\partial E}{\\partial w_1^{i}}\\\\\n",
    "  .\\\\\n",
    "  .\\\\\n",
    "  \\frac{\\partial E}{\\partial w_D^{i}}\n",
    "\\end{bmatrix}\n",
    "  $$\n",
    "  * ### For weight $w_1$:\n",
    "    $$\n",
    "    \\Large  \\Large w_1^{i+1}=w_1^{i}-\\eta \\frac{\\partial E}{\\partial w_1^{i}}\n",
    "    $$\n",
    "    $$\n",
    "    \\Large  \\frac{\\partial E}{\\partial w_1}=-\\left[\\sum_t \\frac{y^t}{p^t} \\frac{\\partial p^t}{\\partial w_1}+\\frac{1-y^t}{1-p^t}\\left(-\\frac{\\partial p^t}{\\partial w_1}\\right)\\right]\n",
    "    $$\n",
    "   \n",
    "    $$\n",
    "    \\mathrm{From}\\ \\ \\ p^t=\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x}^t}} = \\frac{1}{1+e^{-[w_0+w_1x_1^t+w_2x_2^t+....+w_Dx_D^t]}} \\implies \\frac{\\partial p^t}{\\partial w_1}=p^t(1-p^t)x_1^t,\n",
    "    $$\n",
    "    $$\n",
    "    \\Large  \\frac{\\partial E}{\\partial w_1}=-\\left[\\sum_t \\frac{y^t}{p^t} p^t(1-p^t)x_1^t+\\frac{1-y^t}{1-p^t}\\left(-p^t(1-p^t)x_1^t\\right)\\right]\n",
    "    $$\n",
    "    $$\n",
    "    \\Large  \\frac{\\partial E}{\\partial w_1}=-\\left[\\sum_t (y^t -y^tp^t-p^t+y^tp^t)x_1^t\\right]\n",
    "    $$\n",
    "    $$\n",
    "    \\Large  \\frac{\\partial E}{\\partial w_1}=\\sum_t (p^t-y^t)x_1^t\n",
    "    $$\n",
    "    ### Therefore,\n",
    "    $$\n",
    "    \\Large  \\Large w_1^{i+1}=w_1^{i}-\\eta \\sum_t (p^t-y^t)x_1^t\n",
    "    $$\n",
    "     ### Similarly,\n",
    "    $$\n",
    "    \\Large  \\Large w_2^{i+1}=w_2^{i}-\\eta \\sum_t (p^t-y^t)x_2^t\n",
    "    $$\n",
    "    $$\n",
    "    \\Large  \\Large w_3^{i+1}=w_3^{i}-\\eta \\sum_t (p^t-y^t)x_3^t\n",
    "    $$\n",
    "    $$\n",
    "    ..\n",
    "    $$\n",
    "    $$\n",
    "    ..\n",
    "    $$\n",
    "    $$\n",
    "    \\Large  \\Large w_D^{i+1}=w_D^{i}-\\eta \\sum_t (p^t-y^t)x_D^t\n",
    "    $$\n",
    "    ### And,\n",
    "    $$\n",
    "    \\Large  \\Large w_0^{i+1}=w_0^{i}-\\eta \\sum_t (p^t-y^t)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d778e330-9855-4b13-b79a-2155a1e607ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1adfc3f5-a1d5-47a7-94a0-0952fb9564ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateXvector(X):\n",
    "    \"\"\" Taking the original independent variables matrix and add a row of 1 which corresponds to x_0\n",
    "        Parameters:\n",
    "          X:  independent variables matrix\n",
    "        Return value: the matrix that contains all the values in the dataset, not include the outcomes variables. \n",
    "    \"\"\"    \n",
    "    vectorX = np.c_[np.ones((len(X), 1)), X]\n",
    "    return vectorX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c59d5bd-86dc-4266-a7da-6f1bbadf52ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(X):\n",
    "    \"\"\" Generate an initial value of vector θ from the original independent variables matrix\n",
    "         Parameters:\n",
    "          X:  independent variables matrix\n",
    "        Return value: a vector of theta filled with initial guess\n",
    "    \"\"\"\n",
    "    weight = np.random.randn(len(X[0])+1, 1)\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5290dbf-71d3-4dfc-8e9a-ec943b0627c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_function(X):\n",
    "    \"\"\" Calculate the sigmoid value of the inputs\n",
    "         Parameters:\n",
    "          X:  values\n",
    "        Return value: the sigmoid value\n",
    "    \"\"\"\n",
    "    return 1/(1+math.e**(-X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bef4ad1-9617-4870-99e2-8f3185d03a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient_Descent(X,y,learningrate, iterations):\n",
    "    \"\"\" Find the Logistics regression model for the data set\n",
    "         Parameters:\n",
    "          X: independent variables matrix\n",
    "          y: dependent variables matrix\n",
    "          learningrate: learningrate of Gradient Descent\n",
    "          iterations: the number of iterations\n",
    "        Return value: the final theta vector and the plot of cost function\n",
    "    \"\"\"\n",
    "    y_new = np.reshape(y, (len(y), 1))   \n",
    "    cost_lst = []\n",
    "    vectorX = generateXvector(X)\n",
    "    weight = weight_init(X)\n",
    "    m = len(X)\n",
    "    for i in range(iterations):\n",
    "        gradients = 2/m * vectorX.T.dot(sigmoid_function(vectorX.dot(weight)) - y_new)\n",
    "        weight = weight - learningrate * gradients\n",
    "        y_pred = sigmoid_function(vectorX.dot(weight))\n",
    "        cost_value = - np.sum(np.dot(y_new.T,np.log(y_pred)+ np.dot((1-y_new).T,np.log(1-y_pred)))) /(len(y_pred))\n",
    " #Calculate the loss for each training instance\n",
    "        cost_lst.append(cost_value)\n",
    "    plt.plot(np.arange(1,iterations),cost_lst[1:], color = 'red')\n",
    "    plt.title('Cost function Graph')\n",
    "    plt.xlabel('Number of iterations')\n",
    "    plt.ylabel('Cost')\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbd31712-fc80-41e0-800b-ca1924e612a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"]\n",
    "y = (iris[\"target\"] == 0).astype(int) #return 1 if Iris Versicolor, else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffa59a53-3474-43d9-9968-25703bb87a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b0e280a-c2dc-460a-a66a-f1b163685678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column(matrix, i):\n",
    "    \"\"\" Returning all the values in a specific columns\n",
    "         Parameters:\n",
    "          X: the input matrix\n",
    "          i: the column\n",
    "     Return value: an array with desired column\n",
    "    \"\"\"\n",
    "    return [row[i] for row in matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6b452d8-4129-4aa1-8d97-e63c393c3969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_LR(model,X, y_true):\n",
    "    \"\"\" Returning the accuracy score for a training model\n",
    "    \"\"\"\n",
    "    hypo_line = model[0]\n",
    "    for i in range(1,len(model)):\n",
    "        hypo_line = hypo_line + model[i]*column(X,i-1)\n",
    "    y_pred = sigmoid_function(hypo_line)\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] >= 0.5:\n",
    "            y_pred[i] = 1\n",
    "        else:\n",
    "            y_pred[i] = 0\n",
    "    last1 = np.concatenate((y_pred.reshape(len(y_pred),1), y_true.reshape(len(y_true),1)),1)\n",
    "    count = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if last1[i][0] == last1[i][1]:\n",
    "            count = count+1\n",
    "    acc = count/(len(y_true))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67ecfe5e-08c6-49cc-8a3d-3548d89ab9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights_learnt = Gradient_Descent(X_train,y_train,1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8daf0e6-c042-4bab-a1d6-27dfe41d10d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_LR(weights_learnt,X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d125996-8366-4bf1-844b-e7dd29acdf0c",
   "metadata": {},
   "source": [
    "## <span style='color:Blue'>Differences between Linear and Logistic Regression:</span>\n",
    " &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;  <img src=\"images/Lin_Log.png\" width=\"400\" height=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcf16d1-6f1b-42ed-b8f8-5860e80dbb93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
