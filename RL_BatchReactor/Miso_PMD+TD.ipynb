{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0665c6",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Policy Mirror Descent with Temporal Difference Learning on Batch Reactor Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a5b20-a215-4564-aa9f-2ef77b85e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.integrate\n",
    "import gym\n",
    "from gym import spaces\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# Kinetic and physical constants\n",
    "Ad = 4.4e16\n",
    "Ed = 140.06e3\n",
    "Ap = 1.7e11 / 60\n",
    "Ep = 16.9e3 / 0.239\n",
    "deltaHp = -82.2e3\n",
    "UA = 33.3083\n",
    "Qc = 650\n",
    "Qs = 12.41e-2\n",
    "# V will now vary from 4-20 mA (constant removed)\n",
    "Tc = 27\n",
    "Tamb = 27\n",
    "Cpc = 4.184\n",
    "R = 8.3145\n",
    "alpha = 1.212827\n",
    "beta = 0.000267\n",
    "epsilon = 0.5\n",
    "theta = 1.25\n",
    "m1 = 450\n",
    "cp1 = 4.184\n",
    "mjCpj = (18 * 4.184) + (240 * 0.49)\n",
    "cp2 = 187\n",
    "cp3 = 110.58\n",
    "cp4 = 84.95\n",
    "m5 = 220\n",
    "cp5 = 0.49\n",
    "m6 = 7900\n",
    "cp6 = 0.49\n",
    "M0 = 0.7034\n",
    "\n",
    "\n",
    "def br(x, t, u, Ad):\n",
    "    \"\"\"\n",
    "    Batch reactor model\n",
    "    u[0]: Coolant flow (0.1-1.0 LPM)\n",
    "    u[1]: Heater current (4-20 mA, scaled to effective value)\n",
    "    \"\"\"\n",
    "    coolant_flow = u[0]\n",
    "    heater_current_mA = u[1]  # 4-20 mA range\n",
    "    \n",
    "    # Scale mA to effective V (power scaling proportional to current^2)\n",
    "    # Normalize to 0-1 scale for model compatibility by mapping 4-20mA to 0-1\n",
    "    V_effective = ((heater_current_mA - 4) / 16) ** 2\n",
    "    \n",
    "    F = coolant_flow * 16.667\n",
    "    Ii, M, Tr, Tj = x\n",
    "    Ri = Ad * Ii * np.exp(-Ed / (R * (Tr + 273.15)))\n",
    "    Rp = Ap * (Ii ** epsilon) * (M ** theta) * np.exp(-Ep / (R * (Tr + 273.15)))\n",
    "    mrCpr = m1 * cp1 + Ii * cp2 * V_effective + M * cp3 * V_effective + (M0 - M) * cp4 * V_effective + m5 * cp5 + m6 * cp6\n",
    "    Qpr = alpha * (Tr - Tc) ** beta\n",
    "\n",
    "    dy1_dt = -Ri\n",
    "    dy2_dt = -Rp\n",
    "    dy3_dt = (Rp * V_effective * (-deltaHp) - UA * (Tr - Tj) + Qc + Qs - Qpr) / mrCpr\n",
    "    dy4_dt = (UA * (Tr - Tj) - F * Cpc * (Tj - Tc)) / mjCpj\n",
    "\n",
    "    return [dy1_dt, dy2_dt, dy3_dt, dy4_dt]\n",
    "\n",
    "class BR3(gym.Env):\n",
    "    def __init__(self, setpoint_path):\n",
    "        # Two-dimensional action space for coolant flow and heater current\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([0.1, 4.0]),    # Min: 0.1 LPM coolant, 4 mA heater current\n",
    "            high=np.array([1.0, 20.0]),  # Max: 1.0 LPM coolant, 20 mA heater current\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.observation_space = spaces.Box(low=0, high=100, shape=(2,), dtype=np.float32)\n",
    "        self.t = np.linspace(0, 7200, 7201)\n",
    "        self.i = 0\n",
    "\n",
    "        Tr_ref = pd.read_csv(setpoint_path)\n",
    "        self.a1 = Tr_ref.values.tolist()\n",
    "        self.sp = self.a1[self.i][0]\n",
    "        \n",
    "        # Track controls for analysis\n",
    "        self.coolant_flows = []\n",
    "        self.heater_currents = []\n",
    "        \n",
    "        # Error tracking\n",
    "        self.error_history = []\n",
    "        self.prev_error = 0\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        # Process the two control actions\n",
    "        coolant_flow = float(action[0])\n",
    "        heater_current = float(action[1])\n",
    "        \n",
    "        # Ensure actions are within range\n",
    "        coolant_flow = np.clip(coolant_flow, 0.1, 1.0)\n",
    "        heater_current = np.clip(heater_current, 4.0, 20.0)  # 4-20 mA range\n",
    "        \n",
    "        # Store for plotting\n",
    "        self.coolant_flows.append(coolant_flow)\n",
    "        self.heater_currents.append(heater_current)\n",
    "        \n",
    "        # Simulate system for one time step\n",
    "        ts = [self.t[self.i], self.t[self.i + 1]]\n",
    "        y = scipy.integrate.odeint(br, self.y0, ts, args=([coolant_flow, heater_current], Ad))\n",
    "        x = np.round(y, decimals=4)\n",
    "\n",
    "        # Update state variables\n",
    "        self.I, self.M, self.Tr, self.Tj = x[-1]\n",
    "        self.y0 = np.array([self.I, self.M, self.Tr, self.Tj])\n",
    "\n",
    "        # Update setpoint\n",
    "        self.sp = self.a1[self.i][0]\n",
    "        self.i += 1\n",
    "\n",
    "        # Calculate tracking error\n",
    "        error = self.sp - self.Tr\n",
    "        error_derivative = error - self.prev_error\n",
    "        self.prev_error = error\n",
    "        \n",
    "        # Store error history\n",
    "        self.error_history.append(error)\n",
    "        if len(self.error_history) > 10:\n",
    "            self.error_history.pop(0)\n",
    "\n",
    "        # Asymmetric reward function\n",
    "        if error < 0:  # Temperature too high (overshoot)\n",
    "            # Stronger penalty for overshooting\n",
    "            reward = -200 * abs(error)**2 - 50 * abs(error_derivative)\n",
    "        else:  # Temperature too low\n",
    "            reward = -50 * abs(error)**2 - 20 * abs(error_derivative)\n",
    "            \n",
    "        # Reward for precision\n",
    "        if abs(error) < 0.3:\n",
    "            reward += 200\n",
    "        elif abs(error) < 0.8:\n",
    "            reward += 100\n",
    "\n",
    "        done = self.i >= 7200\n",
    "        self.state = (self.Tr, self.sp)\n",
    "        return np.array(self.state), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.I = 4.5e-3\n",
    "        self.M = 0.7034\n",
    "        self.Tr = 45.0\n",
    "        self.Tj = 40.0\n",
    "        self.i = 0\n",
    "        self.sp = self.a1[self.i][0]\n",
    "        self.state = (self.Tr, self.sp)\n",
    "        self.y0 = np.array([self.I, self.M, self.Tr, self.Tj])\n",
    "        \n",
    "        # Reset tracking arrays\n",
    "        self.coolant_flows = []\n",
    "        self.heater_currents = []\n",
    "        self.error_history = []\n",
    "        self.prev_error = 0\n",
    "        \n",
    "        return np.array(self.state)\n",
    "\n",
    "class DualPMD_Controller:\n",
    "    \"\"\"\n",
    "    Dual-output PMD controller for coolant flow and heater current\n",
    "    \"\"\"\n",
    "    def __init__(self, coolant_space, heater_space, lr=0.15, gamma=0.95, tau=0.05):\n",
    "        # Action spaces\n",
    "        self.coolant_space = coolant_space\n",
    "        self.heater_space = heater_space\n",
    "        self.coolant_size = len(coolant_space)\n",
    "        self.heater_size = len(heater_space)\n",
    "        \n",
    "        # Learning parameters\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        # State representation\n",
    "        self.error_bins = 300\n",
    "        self.error_range = 15.0\n",
    "        \n",
    "        # Q-tables for each control\n",
    "        self.Q_coolant = np.zeros((self.error_bins, self.coolant_size))\n",
    "        self.Q_heater = np.zeros((self.error_bins, self.heater_size))\n",
    "        \n",
    "        # Policy parameters\n",
    "        self.theta_coolant = np.zeros((self.error_bins, self.coolant_size))\n",
    "        self.theta_heater = np.zeros((self.error_bins, self.heater_size))\n",
    "        \n",
    "        # Initialize with strategic bias\n",
    "        for s in range(self.error_bins):\n",
    "            error = -self.error_range + s * (2 * self.error_range) / self.error_bins\n",
    "            \n",
    "            if error < 0:  # Temperature too high - more cooling, less heating\n",
    "                # Bias toward higher coolant flow\n",
    "                for a in range(self.coolant_size):\n",
    "                    self.theta_coolant[s, a] = 2.0 * (a / (self.coolant_size - 1))\n",
    "                \n",
    "                # Bias toward lower heater current\n",
    "                for a in range(self.heater_size):\n",
    "                    self.theta_heater[s, a] = 2.0 * (1 - a / (self.heater_size - 1))\n",
    "            \n",
    "            elif error > 0:  # Temperature too low - less cooling, more heating\n",
    "                # Bias toward lower coolant flow\n",
    "                for a in range(self.coolant_size):\n",
    "                    self.theta_coolant[s, a] = 2.0 * (1 - a / (self.coolant_size - 1))\n",
    "                \n",
    "                # Bias toward higher heater current\n",
    "                for a in range(self.heater_size):\n",
    "                    self.theta_heater[s, a] = 2.0 * (a / (self.heater_size - 1))\n",
    "        \n",
    "        # Initialize policy distributions\n",
    "        self.coolant_policy = np.zeros((self.error_bins, self.coolant_size))\n",
    "        self.heater_policy = np.zeros((self.error_bins, self.heater_size))\n",
    "        self.update_policies()\n",
    "        \n",
    "        # Experience buffer\n",
    "        self.buffer = []\n",
    "        self.buffer_size = 20000\n",
    "        \n",
    "        # Action smoothing (reduced for heater to encourage variation)\n",
    "        self.coolant_momentum = 0.5\n",
    "        self.heater_momentum = 0.2  # Low momentum for heater = more variation\n",
    "        self.last_coolant = 0.5\n",
    "        self.last_heater = 12.0  # Midpoint of 4-20 mA range\n",
    "        \n",
    "        # Tracking variables\n",
    "        self.prev_error = 0\n",
    "        self.error_integral = 0\n",
    "        \n",
    "        # Oscillation counters for current\n",
    "        self.oscillation_timer = 0\n",
    "        self.oscillation_direction = 1  # 1 or -1\n",
    "\n",
    "    def get_error_bin(self, error):\n",
    "        \"\"\"Map continuous error to discrete bin\"\"\"\n",
    "        index = int(((error + self.error_range) / (2 * self.error_range)) * self.error_bins)\n",
    "        return np.clip(index, 0, self.error_bins - 1)\n",
    "\n",
    "    def update_policies(self):\n",
    "        \"\"\"Update policies with numerical stability safeguards\"\"\"\n",
    "        for s in range(self.error_bins):\n",
    "            # Coolant policy update with stability fixes\n",
    "            coolant_theta = self.theta_coolant[s]\n",
    "            coolant_max = np.max(coolant_theta)\n",
    "            coolant_exp = np.exp((coolant_theta - coolant_max) / self.tau)\n",
    "            coolant_sum = np.sum(coolant_exp) + 1e-8\n",
    "            self.coolant_policy[s] = coolant_exp / coolant_sum\n",
    "            \n",
    "            # Check for NaN and fix if needed\n",
    "            if np.any(np.isnan(self.coolant_policy[s])):\n",
    "                self.coolant_policy[s] = np.ones(self.coolant_size) / self.coolant_size\n",
    "            \n",
    "            # Heater policy update with stability fixes\n",
    "            heater_theta = self.theta_heater[s]\n",
    "            heater_max = np.max(heater_theta)\n",
    "            heater_exp = np.exp((heater_theta - heater_max) / self.tau)\n",
    "            heater_sum = np.sum(heater_exp) + 1e-8\n",
    "            self.heater_policy[s] = heater_exp / heater_sum\n",
    "            \n",
    "            # Check for NaN and fix if needed\n",
    "            if np.any(np.isnan(self.heater_policy[s])):\n",
    "                self.heater_policy[s] = np.ones(self.heater_size) / self.heater_size\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select actions for both controls with forced oscillation for current\"\"\"\n",
    "        Tr, sp = state\n",
    "        error = sp - Tr\n",
    "        \n",
    "        # PID-like components\n",
    "        error_derivative = error - self.prev_error\n",
    "        self.prev_error = error\n",
    "        self.error_integral = np.clip(self.error_integral + error, -10, 10)\n",
    "        \n",
    "        # Get state bin\n",
    "        error_bin = self.get_error_bin(error)\n",
    "        \n",
    "        # Sample from policies\n",
    "        try:\n",
    "            coolant_idx = np.random.choice(self.coolant_size, p=self.coolant_policy[error_bin])\n",
    "            heater_idx = np.random.choice(self.heater_size, p=self.heater_policy[error_bin])\n",
    "        except:\n",
    "            # Fallback if probabilities are invalid\n",
    "            coolant_idx = np.random.randint(0, self.coolant_size)\n",
    "            heater_idx = np.random.randint(0, self.heater_size)\n",
    "        \n",
    "        # Base actions\n",
    "        raw_coolant = self.coolant_space[coolant_idx]\n",
    "        raw_heater = self.heater_space[heater_idx]\n",
    "        \n",
    "        # Force heater current oscillation to ensure variation (4-20 mA range)\n",
    "        self.oscillation_timer += 1\n",
    "        \n",
    "        # Change direction every 300-600 steps (randomly vary period)\n",
    "        if self.oscillation_timer >= np.random.randint(300, 600):\n",
    "            self.oscillation_direction *= -1\n",
    "            self.oscillation_timer = 0\n",
    "        \n",
    "        # Add sinusoidal component to heater current\n",
    "        oscillation_amplitude = 2.0  # +/- 2 mA\n",
    "        heater_oscillation = oscillation_amplitude * np.sin(self.oscillation_timer / 100 * np.pi)\n",
    "        \n",
    "        # Apply error-based adjustments\n",
    "        if error < 0:  # Tr > sp (too hot)\n",
    "            # Need more cooling, less heating\n",
    "            coolant_adjust = min(0.3, 0.1 + 0.1 * abs(error))\n",
    "            heater_adjust = -min(4.0, 1.0 + 2.0 * abs(error))\n",
    "            \n",
    "            raw_coolant = min(1.0, raw_coolant + coolant_adjust)\n",
    "            raw_heater = max(4.0, raw_heater + heater_adjust)\n",
    "            \n",
    "        else:  # Tr < sp (too cold)\n",
    "            # Need less cooling, more heating\n",
    "            coolant_adjust = -min(0.3, 0.1 + 0.1 * error)\n",
    "            heater_adjust = min(4.0, 1.0 + 2.0 * error)\n",
    "            \n",
    "            raw_coolant = max(0.1, raw_coolant + coolant_adjust)\n",
    "            raw_heater = min(20.0, raw_heater + heater_adjust)\n",
    "        \n",
    "        # Add oscillation to heater current\n",
    "        raw_heater += heater_oscillation\n",
    "        \n",
    "        # Apply momentum for smoother control\n",
    "        smoothed_coolant = self.coolant_momentum * self.last_coolant + (1 - self.coolant_momentum) * raw_coolant\n",
    "        smoothed_heater = self.heater_momentum * self.last_heater + (1 - self.heater_momentum) * raw_heater\n",
    "        \n",
    "        # Ensure within bounds\n",
    "        smoothed_coolant = np.clip(smoothed_coolant, 0.1, 1.0)\n",
    "        smoothed_heater = np.clip(smoothed_heater, 4.0, 20.0)  # 4-20 mA range\n",
    "        \n",
    "        # Store for next step\n",
    "        self.last_coolant = smoothed_coolant\n",
    "        self.last_heater = smoothed_heater\n",
    "        \n",
    "        return np.array([smoothed_coolant, smoothed_heater]), coolant_idx, heater_idx, error_bin\n",
    "\n",
    "    def store_experience(self, error_bin, coolant_idx, heater_idx, reward, next_error_bin, next_coolant_idx, next_heater_idx):\n",
    "        \"\"\"Store experience in buffer\"\"\"\n",
    "        experience = (error_bin, coolant_idx, heater_idx, reward, next_error_bin, next_coolant_idx, next_heater_idx)\n",
    "        if len(self.buffer) >= self.buffer_size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def train(self, batch_size=64):\n",
    "        \"\"\"Train controller using experiences from buffer\"\"\"\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return 0\n",
    "        \n",
    "        # Sample batch\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        batch = [self.buffer[i] for i in indices]\n",
    "        \n",
    "        total_error = 0\n",
    "        for error_bin, coolant_idx, heater_idx, reward, next_error_bin, next_coolant_idx, next_heater_idx in batch:\n",
    "            # Current Q-values\n",
    "            current_q_coolant = self.Q_coolant[error_bin, coolant_idx]\n",
    "            current_q_heater = self.Q_heater[error_bin, heater_idx]\n",
    "            \n",
    "            # Next Q-values\n",
    "            next_q_coolant = self.Q_coolant[next_error_bin, next_coolant_idx]\n",
    "            next_q_heater = self.Q_heater[next_error_bin, next_heater_idx]\n",
    "            \n",
    "            # TD targets\n",
    "            td_target_coolant = reward + self.gamma * next_q_coolant\n",
    "            td_target_heater = reward + self.gamma * next_q_heater\n",
    "            \n",
    "            # TD errors\n",
    "            td_error_coolant = td_target_coolant - current_q_coolant\n",
    "            td_error_heater = td_target_heater - current_q_heater\n",
    "            \n",
    "            # Update Q-values\n",
    "            self.Q_coolant[error_bin, coolant_idx] += self.lr * td_error_coolant\n",
    "            self.Q_heater[error_bin, heater_idx] += self.lr * td_error_heater\n",
    "            \n",
    "            total_error += abs(td_error_coolant) + abs(td_error_heater)\n",
    "        \n",
    "        # Update policies\n",
    "        for s in range(self.error_bins):\n",
    "            # Mirror descent updates with clipping for stability\n",
    "            self.theta_coolant[s] = self.theta_coolant[s] - self.lr * (-self.Q_coolant[s])\n",
    "            self.theta_coolant[s] = np.clip(self.theta_coolant[s], -10, 10)\n",
    "            \n",
    "            self.theta_heater[s] = self.theta_heater[s] - self.lr * (-self.Q_heater[s])\n",
    "            self.theta_heater[s] = np.clip(self.theta_heater[s], -10, 10)\n",
    "            \n",
    "            # Maintain bias for different error cases\n",
    "            error = -self.error_range + s * (2 * self.error_range) / self.error_bins\n",
    "            if error < 0:  # Too hot\n",
    "                # Bias coolant up, heater down\n",
    "                for a in range(self.coolant_size):\n",
    "                    bias = 0.2 * (a / (self.coolant_size - 1))\n",
    "                    self.theta_coolant[s, a] += bias\n",
    "                \n",
    "                for a in range(self.heater_size):\n",
    "                    bias = 0.2 * (1 - a / (self.heater_size - 1))\n",
    "                    self.theta_heater[s, a] += bias\n",
    "            \n",
    "            elif error > 0:  # Too cold\n",
    "                # Bias coolant down, heater up\n",
    "                for a in range(self.coolant_size):\n",
    "                    bias = 0.2 * (1 - a / (self.coolant_size - 1))\n",
    "                    self.theta_coolant[s, a] += bias\n",
    "                \n",
    "                for a in range(self.heater_size):\n",
    "                    bias = 0.2 * (a / (self.heater_size - 1))\n",
    "                    self.theta_heater[s, a] += bias\n",
    "        \n",
    "        # Update policy distributions\n",
    "        self.update_policies()\n",
    "        \n",
    "        return total_error / (batch_size * 2)\n",
    "\n",
    "    def decay_learning_rate(self, factor=0.98):\n",
    "        \"\"\"Decay learning rate\"\"\"\n",
    "        self.lr *= factor\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset controller state\"\"\"\n",
    "        self.prev_error = 0\n",
    "        self.error_integral = 0\n",
    "        self.last_coolant = 0.5\n",
    "        self.last_heater = 12.0  # Midpoint of 4-20 mA range\n",
    "        self.oscillation_timer = 0\n",
    "\n",
    "# Main execution code\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initialize environment and controller\n",
    "env = BR3('Trajectory2.csv')\n",
    "coolant_space = np.linspace(0.1, 1.0, 21)\n",
    "heater_space = np.linspace(4.0, 20.0, 21)  # 4-20 mA range\n",
    "controller = DualPMD_Controller(coolant_space, heater_space)\n",
    "\n",
    "# Training parameters\n",
    "episodes = 15\n",
    "all_rewards = []\n",
    "all_errors = []\n",
    "all_temps = []\n",
    "all_setpoints = []\n",
    "\n",
    "# Training loop\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    controller.reset()\n",
    "    \n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    episode_errors = []\n",
    "    temps = []\n",
    "    setpoints = []\n",
    "    \n",
    "    while not done:\n",
    "        # Select actions\n",
    "        action, coolant_idx, heater_idx, error_bin = controller.select_action(state)\n",
    "        \n",
    "        # Take step in environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Calculate error\n",
    "        Tr, sp = state\n",
    "        temps.append(Tr)\n",
    "        setpoints.append(sp)\n",
    "        error = sp - Tr\n",
    "        abs_error = abs(error)\n",
    "        episode_errors.append(abs_error)\n",
    "        \n",
    "        # Get next action for TD update\n",
    "        next_action, next_coolant_idx, next_heater_idx, next_error_bin = controller.select_action(next_state)\n",
    "        \n",
    "        # Store and learn from experience\n",
    "        controller.store_experience(\n",
    "            error_bin, coolant_idx, heater_idx, reward,\n",
    "            next_error_bin, next_coolant_idx, next_heater_idx\n",
    "        )\n",
    "        \n",
    "        # Train controller\n",
    "        if ep > 0:  # Skip training in first episode to collect experiences\n",
    "            controller.train(batch_size=64)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "    \n",
    "    # Track performance\n",
    "    avg_error = np.mean(episode_errors)\n",
    "    all_rewards.append(episode_reward)\n",
    "    all_errors.append(avg_error)\n",
    "    all_temps.append(temps)\n",
    "    all_setpoints.append(setpoints)\n",
    "    \n",
    "    # Decay learning rate\n",
    "    controller.decay_learning_rate()\n",
    "    \n",
    "    print(f\"Episode {ep+1}: Total Reward = {episode_reward}, Avg Error = {avg_error:.3f}\")\n",
    "\n",
    "# Get final episode data\n",
    "temps = all_temps[-1]\n",
    "setpoints = all_setpoints[-1]\n",
    "\n",
    "# Apply moving average for smoothing\n",
    "def moving_average(data, window_size=50):\n",
    "    return pd.Series(data).rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "coolant_flows = moving_average(env.coolant_flows, 50)\n",
    "heater_currents = moving_average(env.heater_currents, 50)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(14, 12))\n",
    "\n",
    "# Temperature Tracking\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(temps, label=\"Reactor Temperature\")\n",
    "plt.plot(setpoints, label=\"Setpoint\", linestyle=\"--\")\n",
    "plt.ylabel(\"Temperature (°C)\")\n",
    "plt.title(\"Trajectory Tracking - Final Episode\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Coolant Flow Rate\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(coolant_flows, color='green', label=\"Coolant Flow Rate (Smoothed)\")\n",
    "plt.ylabel(\"Flow Rate (LPM)\")\n",
    "plt.title(\"Coolant Flow Rate Over Time - Final Episode\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Tracking Error\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(np.abs(np.array(setpoints) - np.array(temps)), color='red', label=\"Tracking Error\")\n",
    "plt.ylabel(\"Error (°C)\")\n",
    "plt.title(\"Tracking Error Over Time - Final Episode\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Heater Current in mA\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(heater_currents, color='orange', label=\"Heater Current (mA)\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Current (mA)\")\n",
    "plt.title(\"Heater Current Over Time (4-20 mA) - Final Episode\")\n",
    "plt.ylim(3, 21)  # Show full 4-20 mA range\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Two-subplot version matching screenshot\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Temperature tracking\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(temps, 'b-', label='Reactor Temperature')\n",
    "plt.plot(setpoints, 'r--', label='Setpoint')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.title('Trajectory Tracking - Final Episode')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Heater current\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(env.heater_currents, 'orange', label='Heater Current (mA)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Current (mA)')\n",
    "plt.title('Heater Current Variation (4-20 mA)')\n",
    "plt.ylim(3, 21)  # Show full 4-20 mA range\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
